"""
vLLM æ¨ç†æ¨¡å—ï¼ˆClient-Server æ¶æ„ï¼‰

ä½¿ç”¨ vLLM ä½œä¸ºç‹¬ç«‹æœåŠ¡ï¼Œæ’ä»¶é€šè¿‡ OpenAI-compatible API è°ƒç”¨ã€‚
è¿™å½»åº•è§£å†³äº† duplicate template name é”™è¯¯ï¼Œå› ä¸ºæ’ä»¶è¿›ç¨‹ä¸å†å¯¼å…¥ vLLM/torchã€‚

ä½¿ç”¨æ–¹å¼ï¼š
1. ç”¨æˆ·å¯åŠ¨ vLLM æœåŠ¡ï¼ˆç‹¬ç«‹è¿›ç¨‹ï¼‰ï¼š
   vllm serve Qwen/Qwen2.5-7B-Instruct --port 8000

2. æ’ä»¶ä½œä¸ºå®¢æˆ·ç«¯è¿æ¥æœåŠ¡ï¼š
   from src.inference import VLLMInference
   engine = VLLMInference()
   response, metrics = engine.generate("Hello")
"""

import time
import os
from typing import List, Dict, Any, Optional, Tuple
import requests

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAI = None

try:
    from loguru import logger
except ImportError:
    import logging
    logger = logging.getLogger(__name__)

from ..config import get_settings


class VLLMInference:
    """
    vLLM æ¨ç†å¼•æ“ï¼ˆå®¢æˆ·ç«¯æ¨¡å¼ï¼‰
    
    é€šè¿‡ OpenAI-compatible API è¿æ¥åˆ°ç‹¬ç«‹çš„ vLLM æœåŠ¡ã€‚
    æ’ä»¶è¿›ç¨‹ä¸å†å¯¼å…¥ vLLM/torchï¼Œå½»åº•é¿å… duplicate template name é”™è¯¯ã€‚
    """
    
    def __init__(
        self,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        model_path: Optional[str] = None,  # å‘åå…¼å®¹ï¼šæ”¯æŒæ—§å‚æ•°å
        api_key: Optional[str] = None,
        timeout: Optional[float] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“ï¼ˆå®¢æˆ·ç«¯ï¼‰
        
        Args:
            base_url: vLLM æœåŠ¡åœ°å€ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
            model_path: [å·²åºŸå¼ƒ] æ¨¡å‹è·¯å¾„ï¼Œç­‰åŒäº model å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
            api_key: API å¯†é’¥ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼Œæœ¬åœ°è¿è¡Œé€šå¸¸ä¸º "EMPTY"ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        """
        if not OPENAI_AVAILABLE:
            raise ImportError(
                "openai åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install openai>=1.0.0"
            )
        
        settings = get_settings()
        
        # å‘åå…¼å®¹ï¼šæ”¯æŒ model_path å‚æ•°ï¼ˆç­‰åŒäº modelï¼‰
        if model_path and not model:
            model = model_path
            logger.warning(
                "âš ï¸ [VLLM] model_path å‚æ•°å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ model å‚æ•°"
            )
        
        # ä½¿ç”¨é…ç½®æˆ–å‚æ•°
        self.base_url = base_url or settings.vllm_base_url
        self.model = model or settings.vllm_model
        self.api_key = api_key if api_key is not None else settings.vllm_api_key
        self.timeout = timeout if timeout is not None else settings.vllm_timeout
        
        # å…¼å®¹æ—§é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        if not self.model and settings.vllm_model_path:
            self.model = settings.vllm_model_path
            logger.warning(
                "âš ï¸ [VLLM] æ£€æµ‹åˆ°ä½¿ç”¨å·²åºŸå¼ƒçš„ VLLM_MODEL_PATH é…ç½®ï¼Œ"
                "è¯·æ”¹ç”¨ VLLM_MODEL å’Œ VLLM_BASE_URL"
            )
        
        if not self.model:
            raise ValueError(
                "æœªæŒ‡å®šæ¨¡å‹åç§°ã€‚è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® VLLM_MODEL æˆ–ä¼ é€’ model å‚æ•°"
            )
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout,
        )
        
        logger.info(f"ğŸš€ [VLLM Client] å·²è¿æ¥åˆ°æ¨ç†æœåŠ¡: {self.base_url}")
        logger.info(f"   æ¨¡å‹: {self.model}")
        
        # å¥åº·æ£€æŸ¥
        self._health_check()
    
    def _health_check(self) -> None:
        """
        æ£€æŸ¥ vLLM æœåŠ¡æ˜¯å¦å¯ç”¨
        
        Raises:
            RuntimeError: å¦‚æœæœåŠ¡ä¸å¯ç”¨
        """
        try:
            # å°è¯•è°ƒç”¨ /v1/models ç«¯ç‚¹
            response = self.client.models.list()
            available_models = [model.id for model in response.data]
            
            if self.model not in available_models:
                logger.warning(
                    f"âš ï¸ [VLLM Client] æ¨¡å‹ '{self.model}' ä¸åœ¨å¯ç”¨æ¨¡å‹åˆ—è¡¨ä¸­"
                )
                logger.warning(f"   å¯ç”¨æ¨¡å‹: {available_models}")
                logger.warning(
                    "   è¯·ç¡®ä¿ vLLM server å¯åŠ¨æ—¶æŒ‡å®šçš„æ¨¡å‹åç§°ä¸é…ç½®ä¸€è‡´"
                )
            else:
                logger.info(f"âœ“ [VLLM Client] æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡")
        except Exception as e:
            error_msg = str(e).lower()
            if "connection" in error_msg or "refused" in error_msg:
                logger.error("âŒ [VLLM Client] æ— æ³•è¿æ¥åˆ° vLLM æœåŠ¡")
                logger.error("   è¯·ç¡®ä¿å·²å¯åŠ¨ vLLM æœåŠ¡ï¼š")
                logger.error("   vllm serve Qwen/Qwen2.5-7B-Instruct --port 8000")
                logger.error("   æˆ–ä½¿ç”¨å…¶ä»–ç«¯å£ï¼ˆéœ€åœ¨ .env ä¸­è®¾ç½® VLLM_BASE_URLï¼‰")
            else:
                logger.warning(f"âš ï¸ [VLLM Client] å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
                logger.warning("   æœåŠ¡å¯èƒ½ä»åœ¨å¯åŠ¨ä¸­ï¼Œå°†ç»§ç»­å°è¯•...")
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> Tuple[str, Dict[str, Any]]:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: é‡‡æ ·æ¸©åº¦
            **kwargs: å…¶ä»–é‡‡æ ·å‚æ•°ï¼ˆtop_p, top_k ç­‰ï¼‰
        
        Returns:
            (ç”Ÿæˆçš„æ–‡æœ¬, æ€§èƒ½æŒ‡æ ‡)
        """
        # æ„é€ æ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        # ä¼°ç®—è¾“å…¥ tokens æ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼šä¸­æ–‡çº¦ 1.5 tokens/å­—ç¬¦ï¼Œè‹±æ–‡çº¦ 0.5 tokens/å­—ç¬¦ï¼‰
        # ä¸ºäº†å®‰å…¨ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ä¼°è®¡ï¼š2 tokens/å­—ç¬¦ï¼ˆæ··åˆæ–‡æœ¬ï¼‰
        total_input_text = ""
        for msg in messages:
            total_input_text += msg.get("content", "")
        
        # ç²—ç•¥ä¼°ç®—è¾“å…¥ tokensï¼ˆä¿å®ˆä¼°è®¡ï¼š2 tokens/å­—ç¬¦ï¼‰
        estimated_input_tokens = int(len(total_input_text) * 2)
        
        # vLLM æœåŠ¡çš„ max_model_len é»˜è®¤æ˜¯ 1024ï¼ˆä»é…ç½®è¯»å–ï¼Œä½†è¿™é‡Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
        # å®é™…åº”è¯¥ä»æœåŠ¡ç«¯è·å–ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œä½¿ç”¨ 1024 ä½œä¸ºé»˜è®¤å€¼
        max_model_len = 1024  # è¿™æ˜¯ vLLM æœåŠ¡å¯åŠ¨æ—¶çš„ max_model_len
        
        # è®¡ç®—å¯ç”¨çš„ max_tokens
        available_tokens = max_model_len - estimated_input_tokens - 10  # ç•™ 10 tokens ç¼“å†²
        
        # è®¾ç½®é»˜è®¤ max_tokensï¼ˆmax_model_len=1024 æ—¶ï¼Œå¯ä»¥è®¾ç½®æ›´å¤§çš„é»˜è®¤å€¼ï¼‰
        default_max_tokens = min(512, max(50, available_tokens))  # è‡³å°‘ 50ï¼Œæœ€å¤š 512ï¼Œä½†ä¸èƒ½è¶…è¿‡å¯ç”¨å€¼
        
        if max_tokens is None:
            max_tokens = default_max_tokens
        else:
            # ç¡®ä¿ max_tokens ä¸ä¼šè¶…è¿‡å¯ç”¨å€¼ï¼Œä½†å…è®¸æœ€å¤§åˆ° 512
            max_tokens = min(max_tokens, min(512, available_tokens))
        
        # å¦‚æœè®¡ç®—å‡ºçš„ max_tokens å¤ªå°ï¼Œè‡³å°‘è®¾ä¸º 50
        max_tokens = max(50, max_tokens)
        
        logger.debug(f"è¾“å…¥ tokens ä¼°ç®—: {estimated_input_tokens}, å¯ç”¨ tokens: {available_tokens}, è®¾ç½® max_tokens: {max_tokens}")
        
        request_params = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature if temperature is not None else 0.7,
            "max_tokens": max_tokens,
        }
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        if "top_p" in kwargs:
            request_params["top_p"] = kwargs["top_p"]
        if "top_k" in kwargs:
            request_params["top_k"] = kwargs["top_k"]
        
        # ç”Ÿæˆï¼ˆæµ‹é‡å»¶è¿Ÿï¼‰
        start_time = time.time()
        first_token_time = None
        
        try:
            logger.debug(f"å¼€å§‹ç”Ÿæˆï¼Œæç¤ºé•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.debug(f"è¯·æ±‚å‚æ•°: model={self.model}, max_tokens={max_tokens}, timeout={self.timeout}")
            
            # è°ƒç”¨ OpenAI APIï¼ˆæ³¨æ„ï¼šé¦–æ¬¡æ¨ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´é¢„çƒ­ï¼‰
            logger.info(f"â³ [VLLM Client] æ­£åœ¨ç”Ÿæˆï¼ˆé¦–æ¬¡æ¨ç†å¯èƒ½éœ€è¦é¢„çƒ­ï¼Œè¯·è€å¿ƒç­‰å¾…...ï¼‰")
            response = self.client.chat.completions.create(**request_params)
            
            generation_time = time.time() - start_time
            logger.debug(f"ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time:.2f} ç§’")
            
            # æå–ç»“æœ
            if response.choices and len(response.choices) > 0:
                generated_text = response.choices[0].message.content
                
                # è®¡ç®— tokensï¼ˆå¦‚æœå¯ç”¨ï¼‰
                tokens_generated = 0
                if hasattr(response, 'usage') and response.usage:
                    tokens_generated = response.usage.completion_tokens or 0
                else:
                    # ç²—ç•¥ä¼°è®¡ï¼šå¹³å‡æ¯ä¸ªå­—ç¬¦çº¦ 0.25 tokensï¼ˆä¸­æ–‡ï¼‰æˆ– 0.5 tokensï¼ˆè‹±æ–‡ï¼‰
                    tokens_generated = int(len(generated_text) * 0.4)
                
                # è®¡ç®—ååé‡
                tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
                
                # ä¼°ç®— TTFTï¼ˆé¦–æ¬¡ token å»¶è¿Ÿï¼‰
                # OpenAI API ä¸ç›´æ¥æä¾› TTFTï¼Œæˆ‘ä»¬ä½¿ç”¨æ€»æ—¶é—´çš„ 10-15% ä½œä¸ºä¼°è®¡
                ttft = generation_time * 0.12  # ç²—ç•¥ä¼°è®¡
                
                metrics = {
                    "ttft": ttft,
                    "total_time": generation_time,
                    "tokens_generated": tokens_generated,
                    "tokens_per_second": tokens_per_second,
                }
                
                return generated_text, metrics
            else:
                raise ValueError("ç”Ÿæˆç»“æœä¸ºç©º")
        
        except Exception as e:
            error_msg = str(e).lower()
            if "connection" in error_msg or "refused" in error_msg:
                logger.error("âŒ [VLLM Client] æ— æ³•è¿æ¥åˆ° vLLM æœåŠ¡")
                logger.error("   è¯·ç¡®ä¿å·²å¯åŠ¨ vLLM æœåŠ¡ï¼š")
                logger.error("   vllm serve Qwen/Qwen2.5-7B-Instruct --port 8000")
            elif "timeout" in error_msg:
                logger.error(f"âŒ [VLLM Client] è¯·æ±‚è¶…æ—¶ï¼ˆ{self.timeout} ç§’ï¼‰")
                logger.error("   å¯èƒ½æ˜¯æ¨¡å‹å“åº”å¤ªæ…¢ï¼Œå°è¯•å¢åŠ  VLLM_TIMEOUT æˆ–å‡å°‘ max_tokens")
            else:
                logger.error(f"âŒ [VLLM Client] ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def generate_batch(
        self,
        prompts: List[str],
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> List[Tuple[str, Dict[str, Any]]]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompts: æç¤ºåˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            **kwargs: é‡‡æ ·å‚æ•°
        
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        results = []
        start_time = time.time()
        
        # ä¸²è¡Œå¤„ç†ï¼ˆvLLM server æœ¬èº«æ”¯æŒå¹¶å‘ï¼Œä½†è¿™é‡Œç®€åŒ–å®ç°ï¼‰
        for prompt in prompts:
            try:
                text, metrics = self.generate(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    **kwargs
                )
                results.append((text, metrics))
            except Exception as e:
                logger.error(f"æ‰¹é‡ç”Ÿæˆä¸­å•ä¸ªè¯·æ±‚å¤±è´¥: {e}")
                results.append(("", {}))
        
        total_time = time.time() - start_time
        
        # æ›´æ–°æ¯ä¸ªç»“æœçš„ metricsï¼ˆä½¿ç”¨å¹³å‡æ—¶é—´ï¼‰
        for i, (text, metrics) in enumerate(results):
            if metrics:
                metrics["total_time"] = total_time / len(prompts)
                metrics["tokens_per_second"] = (
                    metrics.get("tokens_generated", 0) / metrics["total_time"]
                    if metrics["total_time"] > 0 else 0
                )
        
        return results
    
    def generate_stream(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> Any:
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬ï¼ˆç”Ÿæˆå™¨ï¼‰
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: é‡‡æ ·æ¸©åº¦
            **kwargs: å…¶ä»–é‡‡æ ·å‚æ•°
        
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        # æ„é€ æ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        request_params = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature if temperature is not None else 0.7,
            "max_tokens": max_tokens if max_tokens is not None else 512,
            "stream": True,  # å¯ç”¨æµå¼è¾“å‡º
        }
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        if "top_p" in kwargs:
            request_params["top_p"] = kwargs["top_p"]
        if "top_k" in kwargs:
            request_params["top_k"] = kwargs["top_k"]
        
        try:
            stream = self.client.chat.completions.create(**request_params)
            
            for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta and delta.content:
                        yield delta.content
        
        except Exception as e:
            error_msg = str(e).lower()
            if "connection" in error_msg or "refused" in error_msg:
                logger.error("âŒ [VLLM Client] æ— æ³•è¿æ¥åˆ° vLLM æœåŠ¡")
                logger.error("   è¯·ç¡®ä¿å·²å¯åŠ¨ vLLM æœåŠ¡")
            else:
                logger.error(f"âŒ [VLLM Client] æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ¨ç†å¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "base_url": self.base_url,
            "model": self.model,
            "timeout": self.timeout,
            "mode": "client-server",
        }


# å…¼å®¹æ€§ï¼šä¿æŒ VLLM_AVAILABLE å˜é‡ï¼ˆç°åœ¨æ€»æ˜¯ Trueï¼Œå› ä¸ºåªéœ€è¦ openai åº“ï¼‰
VLLM_AVAILABLE = OPENAI_AVAILABLE
