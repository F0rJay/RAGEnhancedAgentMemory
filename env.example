# 向量数据库配置
# Qdrant 配置
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY: 本地部署留空，云端部署从 https://cloud.qdrant.io/ 获取
QDRANT_API_KEY=
QDRANT_COLLECTION_NAME=agent_memory

# Chroma 配置 (如果使用 Chroma 作为替代)
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_COLLECTION_NAME=agent_memory

# 向量数据库选择: qdrant 或 chroma
VECTOR_DB=qdrant

# 关系型数据库配置 (PostgreSQL)
# 如果未安装 PostgreSQL，此配置可以暂时留空或跳过
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=agent_memory
POSTGRES_USER=postgres
# POSTGRES_PASSWORD: PostgreSQL 密码（示例密码：rag_memory_2024）
# ⚠️ 生产环境请使用强密码！这个密码仅用于开发测试
# 使用 Docker 启动: docker run -e POSTGRES_PASSWORD=rag_memory_2024 -e POSTGRES_DB=agent_memory -p 5432:5432 -d postgres:15
POSTGRES_PASSWORD=rag_memory_2024

# 模型配置
# 嵌入模型 (用于向量化文本)
EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5
EMBEDDING_DIM=1024
EMBEDDING_DEVICE=cuda

# 重排序模型
RERANK_MODEL=BAAI/bge-reranker-large
RERANK_TOP_K=5

# vLLM 配置 (如果使用本地推理)
# VLLM_MODEL_PATH: 本地模型路径（相对路径或绝对路径）或 HuggingFace 模型 ID
# 推荐使用 7B 模型以在 31GB GPU 上运行：
#   - 7B 模型: Qwen/Qwen2.5-7B-Instruct (推荐，适合 31GB GPU)
#   - 本地模型: ./models/Qwen2.5-7B-Instruct
# 如果不使用本地推理，可以留空，使用云端 API
VLLM_MODEL_PATH=Qwen/Qwen2.5-7B-Instruct
# GPU 内存使用率（31GB GPU 建议 0.4，确保有足够 KV cache 内存）
VLLM_GPU_MEMORY_UTILIZATION=0.4
# 最大模型长度（31GB GPU 建议 512-1024，降低以节省内存）
VLLM_MAX_MODEL_LEN=512

# 镜像源配置
# HF_ENDPOINT: HuggingFace 镜像源（可选）
# 国内用户推荐使用: https://hf-mirror.com
# 不使用镜像则留空或使用官方源: https://huggingface.co
HF_ENDPOINT=

# API 密钥 (如使用云端服务)
# OPENAI_API_KEY: 从 https://platform.openai.com/api-keys 获取（可选）
OPENAI_API_KEY=
# ANTHROPIC_API_KEY: 从 https://console.anthropic.com/ 获取（可选）
ANTHROPIC_API_KEY=

# 记忆系统配置
SHORT_TERM_THRESHOLD=10
LONG_TERM_TRIGGER=0.7
RERANK_TOP_K=5

# LangGraph 配置
CHECKPOINT_DIR=./checkpoints
ENABLE_CHECKPOINTING=true

# 日志配置
LOG_LEVEL=INFO
LOG_FILE=./logs/agent_memory.log

# 性能配置
ASYNC_BATCH_SIZE=32
MAX_CONCURRENT_REQUESTS=10
