# 向量数据库配置
# Qdrant 配置
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY: 本地部署留空，云端部署从 https://cloud.qdrant.io/ 获取
QDRANT_API_KEY=
QDRANT_COLLECTION_NAME=agent_memory

# Chroma 配置 (如果使用 Chroma 作为替代)
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_COLLECTION_NAME=agent_memory

# 向量数据库选择: qdrant 或 chroma
VECTOR_DB=qdrant

# 关系型数据库配置 (PostgreSQL)
# 如果未安装 PostgreSQL，此配置可以暂时留空或跳过
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=agent_memory
POSTGRES_USER=postgres
# POSTGRES_PASSWORD: PostgreSQL 密码（示例密码：rag_memory_2024）
# ⚠️ 生产环境请使用强密码！这个密码仅用于开发测试
# 使用 Docker 启动: docker run -e POSTGRES_PASSWORD=rag_memory_2024 -e POSTGRES_DB=agent_memory -p 5432:5432 -d postgres:15
POSTGRES_PASSWORD=rag_memory_2024

# 模型配置
# 嵌入模型 (用于向量化文本)
EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5
EMBEDDING_DIM=1024
EMBEDDING_DEVICE=cuda

# 重排序模型
RERANK_MODEL=BAAI/bge-reranker-large
RERANK_TOP_K=5

# vLLM 配置 (Client-Server 架构)
# 注意：vLLM 现在作为独立服务运行，插件通过 HTTP API 调用
# 
# 1. 启动 vLLM 服务（在单独终端运行）：
#    # 使用本地模型（推荐）：
#    vllm serve /path/to/your/local/model --port 8000
#    # 或使用 HuggingFace ID（会自动检测本地缓存）：
#    vllm serve Qwen/Qwen2.5-7B-Instruct --port 8000
#    或使用启动脚本：
#    python scripts/launch_vllm_server.py --model /path/to/your/local/model --port 8000
#
# 2. 配置插件连接信息：
# VLLM_BASE_URL: vLLM 服务地址（OpenAI-compatible API endpoint）
VLLM_BASE_URL=http://localhost:8000/v1
# VLLM_MODEL: 模型路径或名称（本地路径或 HuggingFace ID，必须与 vLLM server 启动时指定的模型一致）
# 示例：
#   - 本地模型: /path/to/your/local/model
#   - HuggingFace ID: Qwen/Qwen2.5-7B-Instruct
#   留空则使用 vLLM server 启动时指定的模型
VLLM_MODEL=
# VLLM_API_KEY: API 密钥（本地运行通常不需要，设为 EMPTY）
VLLM_API_KEY=EMPTY
# VLLM_TIMEOUT: 请求超时时间（秒）
VLLM_TIMEOUT=300.0
#
# [已废弃] 以下配置项保留用于向后兼容，但不再使用：
# VLLM_MODEL_PATH: 已废弃，请使用 VLLM_MODEL
# VLLM_GPU_MEMORY_UTILIZATION: 已废弃，现在由 vLLM server 启动参数控制
# VLLM_MAX_MODEL_LEN: 已废弃，现在由 vLLM server 启动参数控制

# 镜像源配置
# HF_ENDPOINT: HuggingFace 镜像源（可选）
# 国内用户推荐使用: https://hf-mirror.com
# 不使用镜像则留空或使用官方源: https://huggingface.co
HF_ENDPOINT=

# API 密钥 (如使用云端服务)
# OPENAI_API_KEY: 从 https://platform.openai.com/api-keys 获取（可选）
OPENAI_API_KEY=
# ANTHROPIC_API_KEY: 从 https://console.anthropic.com/ 获取（可选）
ANTHROPIC_API_KEY=
# DEEPSEEK_API_KEY: 从 https://platform.deepseek.com/api_keys 获取（可选，用于 auto_player.py）
DEEPSEEK_API_KEY=
# DEEPSEEK_BASE_URL: DeepSeek API 基础 URL（默认: https://api.deepseek.com）
DEEPSEEK_BASE_URL=https://api.deepseek.com

# 记忆系统配置
SHORT_TERM_THRESHOLD=10
LONG_TERM_TRIGGER=0.7
RERANK_TOP_K=5

# LangGraph 配置
CHECKPOINT_DIR=./checkpoints
ENABLE_CHECKPOINTING=true

# 日志配置
LOG_LEVEL=INFO
LOG_FILE=./logs/agent_memory.log

# 性能配置
ASYNC_BATCH_SIZE=32
MAX_CONCURRENT_REQUESTS=10
