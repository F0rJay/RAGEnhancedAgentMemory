"""
Auto-Player: å…¨è‡ªåŠ¨ Agent äº¤äº’éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯ï¼švLLM æ¨ç†é€Ÿåº¦ã€å­˜å‚¨å»é‡æ•ˆæœã€é•¿ç¨‹è®°å¿†å¬å›èƒ½åŠ›
"""

# å¿…é¡»åœ¨å¯¼å…¥ä»»ä½•å…¶ä»–æ¨¡å—ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿å­è¿›ç¨‹ç»§æ‰¿
# è§£å†³ vLLM å­è¿›ç¨‹ä¸­ torch.compile é‡å¤æ³¨å†Œçš„é—®é¢˜
import os
# ç¦ç”¨ torch.compile ä»¥é¿å…å­è¿›ç¨‹ä¸­é‡å¤æ³¨å†Œæ¨¡æ¿
os.environ["TORCH_COMPILE_DISABLE"] = "1"
# å°è¯•å¼ºåˆ¶ä½¿ç”¨ fork æ–¹æ³•è€Œä¸æ˜¯ spawnï¼ˆå¦‚æœ CUDA æœªåˆå§‹åŒ–ï¼‰
# æ³¨æ„ï¼šå¦‚æœ CUDA å·²åˆå§‹åŒ–ï¼ŒvLLM ä¼šå¼ºåˆ¶ä½¿ç”¨ spawnï¼Œè¿™ä¸ªè®¾ç½®å¯èƒ½æ— æ•ˆ
if "VLLM_WORKER_MULTIPROC_METHOD" not in os.environ:
    # å°è¯•ä½¿ç”¨ forkï¼ˆå¦‚æœå¯èƒ½ï¼‰ï¼Œé¿å…å­è¿›ç¨‹é‡æ–°å¯¼å…¥æ¨¡å—
    os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "fork"

import sys
import time
import uuid
import statistics
from pathlib import Path
from typing import List, Dict, Any, Optional
from rich.console import Console
from rich.table import Table
from rich.progress import track

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åœ¨å¯¼å…¥ä»»ä½•åº“ä¹‹å‰è®¾ç½® HuggingFace é•œåƒæºï¼ˆå¦‚æœé…ç½®äº†ï¼‰
# è¿™ç¡®ä¿ huggingface_hub å’Œ transformers ä½¿ç”¨æ­£ç¡®çš„é•œåƒ
try:
    from dotenv import load_dotenv
    load_dotenv(project_root / ".env")
    hf_endpoint = os.getenv("HF_ENDPOINT")
    if hf_endpoint:
        os.environ["HF_ENDPOINT"] = hf_endpoint
        # å°è¯•ä½¿ç”¨ huggingface_hub çš„ set_endpointï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            from huggingface_hub import set_endpoint
            set_endpoint(hf_endpoint)
        except ImportError:
            pass
except Exception:
    pass

try:
    from src.config import get_settings
    from src.core import RAGEnhancedAgentMemory
    from src.memory.long_term import LongTermMemory
except ImportError as e:
    print(f"ç¯å¢ƒåŠ è½½å¤±è´¥: {e}")
    sys.exit(1)

# å¯¼å…¥æœ¬åœ°æ¨ç†å¼•æ“ï¼ˆåˆå¹¶å¯¼å…¥ï¼Œé¿å…é‡å¤å¯¼å…¥å¯¼è‡´çš„é—®é¢˜ï¼‰
VLLM_IMPORT_ERROR = None
VLLM_AVAILABLE = False
VLLMInference = None
HAS_BASELINE = False
BaselineInference = None
TRANSFORMERS_AVAILABLE = False

try:
    # åªå¯¼å…¥ä¸€æ¬¡ src.inferenceï¼Œé¿å…é‡å¤å¯¼å…¥å¯¼è‡´ vLLM æ¨¡å—å†²çª
    from src.inference import (
        VLLMInference, 
        VLLM_AVAILABLE,
        BaselineInference, 
        TRANSFORMERS_AVAILABLE
    )
    HAS_BASELINE = TRANSFORMERS_AVAILABLE
except ImportError as e:
    VLLM_IMPORT_ERROR = str(e)
    # å¦‚æœæ•´ä½“å¯¼å…¥å¤±è´¥ï¼Œå°è¯•åˆ†åˆ«å¯¼å…¥
    try:
        from src.inference import VLLMInference, VLLM_AVAILABLE
    except ImportError:
        pass
    try:
        from src.inference import BaselineInference, TRANSFORMERS_AVAILABLE
        HAS_BASELINE = TRANSFORMERS_AVAILABLE
    except ImportError:
        pass

# åˆå§‹åŒ– Rich æ§åˆ¶å°
console = Console()

class AutoPlayer:
    def __init__(self, model_path: Optional[str] = None, use_baseline: bool = False):
        """
        åˆå§‹åŒ– Auto-Player
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸º Noneï¼Œä»é…ç½®è¯»å–ï¼‰
            use_baseline: æ˜¯å¦ä½¿ç”¨åŸºçº¿æ¨ç†å¼•æ“ï¼ˆFalse ä½¿ç”¨ vLLMï¼ŒTrue ä½¿ç”¨ Baselineï¼‰
        """
        self.settings = get_settings()
        self.session_id = f"auto_player_{uuid.uuid4().hex[:8]}"
        
        # å»¶è¿Ÿ CUDA åˆå§‹åŒ–ï¼šå…ˆåˆå§‹åŒ– vLLMï¼Œå†åˆå§‹åŒ–ä¼šä½¿ç”¨ CUDA çš„ç»„ä»¶ï¼ˆembedding/rerankerï¼‰
        # è¿™æ ·å¯ä»¥é¿å… vLLM å­è¿›ç¨‹ä¸­çš„ duplicate template name é”™è¯¯
        # åŸå› ï¼šå¦‚æœ CUDA åœ¨ä¸»è¿›ç¨‹ä¸­å·²åˆå§‹åŒ–ï¼ŒvLLM å¿…é¡»ä½¿ç”¨ spawn æ–¹æ³•å¯åŠ¨å­è¿›ç¨‹
        # spawn ä¼šé‡æ–°å¯¼å…¥æ‰€æœ‰æ¨¡å—ï¼Œå¯¼è‡´ @torch.compile è£…é¥°å™¨é‡å¤æ³¨å†Œ
        
        # åˆå§‹åŒ–æ¨ç†å¼•æ“é…ç½®
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ model_pathï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        # æ³¨æ„ï¼šå¯¹äº vLLM Client-Server æ¶æ„ï¼Œmodel_path ç”¨äºæŒ‡å®šæ¨¡å‹åç§°ï¼ˆå¦‚æœé…ç½®ä¸­æœªè®¾ç½®ï¼‰
        if model_path:
            self.model_path = model_path
        else:
            # ä¼˜å…ˆä½¿ç”¨æ–°çš„é…ç½®é¡¹
            self.model_path = self.settings.vllm_model or self.settings.vllm_model_path
        
        self.use_baseline = use_baseline
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ¨¡å‹è·¯å¾„ï¼ŒæŠ¥é”™æç¤ºç”¨æˆ·é…ç½®
        # æ³¨æ„ï¼šå¯¹äº Client-Server æ¶æ„ï¼Œéœ€è¦ç”¨æˆ·æ˜ç¡®æŒ‡å®šæ¨¡å‹åç§°
        if not self.model_path:
            console.print("[red]âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„æœªè®¾ç½®[/red]")
            console.print("[yellow]æç¤º: è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€æŒ‡å®šæ¨¡å‹ï¼š[/yellow]")
            console.print("[dim]  1. å‘½ä»¤è¡Œå‚æ•°: --model-path <model-name>[/dim]")
            console.print("[dim]  2. ç¯å¢ƒå˜é‡: åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® VLLM_MODEL=<model-name>[/dim]")
            console.print("[dim]  3. å¦‚æœä½¿ç”¨ DeepSeek APIï¼Œè®¾ç½® VLLM_MODEL=deepseek-chat[/dim]")
            console.print("[dim]  4. å¦‚æœä½¿ç”¨æœ¬åœ° vLLM æœåŠ¡ï¼Œè®¾ç½® VLLM_MODEL ä¸æœåŠ¡å¯åŠ¨æ—¶æŒ‡å®šçš„æ¨¡å‹ä¸€è‡´[/dim]")
            raise ValueError("æ¨¡å‹è·¯å¾„æœªè®¾ç½®ï¼Œæ— æ³•åˆå§‹åŒ–æ¨ç†å¼•æ“")
        
        # ç°åœ¨ self.model_path ä¸€å®šæœ‰å€¼ï¼Œç»§ç»­åˆå§‹åŒ–æ¨ç†å¼•æ“
        if use_baseline:
            # ä½¿ç”¨åŸºçº¿æ¨ç†å¼•æ“ï¼ˆTransformersï¼‰
            if not HAS_BASELINE:
                console.print("[red]âŒ transformers æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨åŸºçº¿æ¨ç†[/red]")
                console.print("[dim]æç¤º: å®‰è£… transformers: pip install transformers torch[/dim]")
                self.inference_engine = None
                self.use_llm = False
            else:
                try:
                    console.print(f"[yellow]ğŸ“Š ä½¿ç”¨åŸºçº¿æ¨ç†å¼•æ“ï¼ˆTransformersï¼‰[/yellow]")
                    self.inference_engine = BaselineInference(model_path=self.model_path)
                    self.use_llm = True
                except Exception as e:
                    console.print(f"[red]âŒ åŸºçº¿æ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}[/red]")
                    import traceback
                    traceback.print_exc()
                    self.inference_engine = None
                    self.use_llm = False
        else:
            # å¢å¼ºç‰ˆå¿…é¡»ä½¿ç”¨ vLLMï¼ˆClient-Server æ¶æ„ï¼‰
            if not VLLM_AVAILABLE:
                console.print("[red]âŒ vLLM å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå¢å¼ºç‰ˆå¿…é¡»ä½¿ç”¨ vLLM[/red]")
                console.print(f"[red]é”™è¯¯åŸå› : æ— æ³•å¯¼å…¥ openai åº“æˆ– VLLMInference ç±»[/red]")
                if VLLM_IMPORT_ERROR:
                    console.print(f"[dim]å¯¼å…¥é”™è¯¯è¯¦æƒ…: {VLLM_IMPORT_ERROR}[/dim]")
                
                console.print("[dim]è§£å†³æ–¹æ¡ˆ:[/dim]")
                console.print("[dim]  1. å®‰è£… openai åº“: pip install openai>=1.0.0[/dim]")
                console.print("[dim]  2. é…ç½® vLLM æœåŠ¡åœ°å€ï¼ˆ.env æ–‡ä»¶ä¸­çš„ VLLM_BASE_URLï¼‰[/dim]")
                console.print("[dim]  3. å¦‚æœä½¿ç”¨æœ¬åœ° vLLM æœåŠ¡ï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡:[/dim]")
                console.print("[dim]     python scripts/launch_vllm_server.py --model <model-path>[/dim]")
                console.print("[dim]  4. å¦‚æœä½¿ç”¨äº‘ç«¯ APIï¼ˆå¦‚ DeepSeekï¼‰ï¼Œè¯·é…ç½® VLLM_BASE_URL å’Œ VLLM_API_KEY[/dim]")
                console.print("\n[red]âŒ å¢å¼ºç‰ˆå¿…é¡»ä½¿ç”¨ vLLMï¼Œç¨‹åºé€€å‡º[/red]")
                sys.exit(1)
            elif not VLLMInference:
                console.print("[red]âŒ vLLM æ¨ç†å¼•æ“ç±»ä¸å¯ç”¨[/red]")
                console.print("[dim]é”™è¯¯åŸå› : VLLMInference ç±»å¯¼å…¥å¤±è´¥[/dim]")
                self.inference_engine = None
                self.use_llm = False
            else:
                try:
                    console.print(f"[green]ğŸš€ ä½¿ç”¨ vLLM æ¨ç†å¼•æ“ï¼ˆClient-Server æ¶æ„ï¼‰[/green]")
                    # Client-Server æ¶æ„ï¼šå¦‚æœæä¾›äº† model_pathï¼Œä½œä¸ºæ¨¡å‹åç§°ä¼ é€’
                    # å¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
                    if self.model_path:
                        console.print(f"[dim]æ¨¡å‹åç§°: {self.model_path}[/dim]")
                        self.inference_engine = VLLMInference(model=self.model_path)
                    else:
                        console.print(f"[dim]ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹è®¾ç½®[/dim]")
                        self.inference_engine = VLLMInference()
                    console.print(f"[dim]æœåŠ¡åœ°å€: {self.inference_engine.base_url}[/dim]")
                    console.print(f"[dim]æ¨¡å‹: {self.inference_engine.model}[/dim]")
                    self.use_llm = True
                except ImportError as e:
                    console.print(f"[red]âŒ vLLM åˆå§‹åŒ–å¤±è´¥: å¯¼å…¥é”™è¯¯[/red]")
                    console.print(f"[red]é”™è¯¯è¯¦æƒ…: {e}[/red]")
                    console.print("[dim]å¯èƒ½çš„åŸå› :[/dim]")
                    console.print("[dim]  1. openai åº“æœªå®‰è£…: pip install openai>=1.0.0[/dim]")
                    console.print("[dim]  2. æ¨¡å—å¯¼å…¥å¤±è´¥[/dim]")
                    import traceback
                    traceback.print_exc()
                    console.print("\n[red]âŒ å¢å¼ºç‰ˆå¿…é¡»ä½¿ç”¨ vLLMï¼Œç¨‹åºé€€å‡º[/red]")
                    sys.exit(1)
                except ValueError as e:
                    console.print(f"[red]âŒ vLLM åˆå§‹åŒ–å¤±è´¥: é…ç½®é”™è¯¯[/red]")
                    console.print(f"[red]é”™è¯¯è¯¦æƒ…: {e}[/red]")
                    console.print("[dim]å¯èƒ½çš„åŸå› :[/dim]")
                    console.print("[dim]  1. æœªé…ç½® VLLM_BASE_URL æˆ– VLLM_MODEL[/dim]")
                    console.print("[dim]  2. æœåŠ¡åœ°å€æ ¼å¼é”™è¯¯[/dim]")
                    console.print("[dim]è§£å†³æ–¹æ¡ˆ:[/dim]")
                    console.print("[dim]  1. åœ¨ .env æ–‡ä»¶ä¸­é…ç½® VLLM_BASE_URLï¼ˆå¦‚ http://localhost:8000/v1ï¼‰[/dim]")
                    console.print("[dim]  2. é…ç½® VLLM_MODELï¼ˆæ¨¡å‹åç§°ï¼‰[/dim]")
                    console.print("[dim]  3. å¦‚æœä½¿ç”¨æœ¬åœ°æœåŠ¡ï¼Œå…ˆå¯åŠ¨: python scripts/launch_vllm_server.py[/dim]")
                    import traceback
                    traceback.print_exc()
                    console.print("\n[red]âŒ å¢å¼ºç‰ˆå¿…é¡»ä½¿ç”¨ vLLMï¼Œç¨‹åºé€€å‡º[/red]")
                    sys.exit(1)
                except RuntimeError as e:
                    console.print(f"[red]âŒ vLLM åˆå§‹åŒ–å¤±è´¥: è¿è¡Œæ—¶é”™è¯¯[/red]")
                    console.print(f"[red]é”™è¯¯è¯¦æƒ…: {e}[/red]")
                    
                    error_str = str(e).lower()
                    if "connection" in error_str or "refused" in error_str:
                        console.print("[yellow]âš ï¸  æ— æ³•è¿æ¥åˆ° vLLM æœåŠ¡[/yellow]")
                        console.print("[dim]å¯èƒ½çš„åŸå› :[/dim]")
                        console.print("[dim]  1. vLLM æœåŠ¡æœªå¯åŠ¨[/dim]")
                        console.print("[dim]  2. æœåŠ¡åœ°å€é…ç½®é”™è¯¯[/dim]")
                        console.print("[dim]  3. ç½‘ç»œè¿æ¥é—®é¢˜[/dim]")
                        console.print("[dim]è§£å†³æ–¹æ¡ˆ:[/dim]")
                        console.print("[dim]  1. å¯åŠ¨æœ¬åœ° vLLM æœåŠ¡: python scripts/launch_vllm_server.py[/dim]")
                        console.print("[dim]  2. æˆ–é…ç½®äº‘ç«¯ API åœ°å€ï¼ˆå¦‚ DeepSeek APIï¼‰[/dim]")
                        console.print("[dim]  3. æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ VLLM_BASE_URL é…ç½®[/dim]")
                    elif "authentication" in error_str or "api key" in error_str:
                        console.print("[yellow]âš ï¸  API å¯†é’¥é”™è¯¯æˆ–æœªæˆæƒ[/yellow]")
                        console.print("[dim]è§£å†³æ–¹æ¡ˆ:[/dim]")
                        console.print("[dim]  1. æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ VLLM_API_KEY[/dim]")
                        console.print("[dim]  2. å¦‚æœä½¿ç”¨æœ¬åœ°æœåŠ¡ï¼Œé€šå¸¸è®¾ä¸º EMPTY[/dim]")
                    else:
                        console.print("[dim]å¯èƒ½çš„åŸå› :[/dim]")
                        console.print("[dim]  1. æœåŠ¡ä¸å¯ç”¨æˆ–å“åº”è¶…æ—¶[/dim]")
                        console.print("[dim]  2. æ¨¡å‹æœªæ‰¾åˆ°æˆ–æœåŠ¡ä¸æ”¯æŒ[/dim]")
                    
                    import traceback
                    traceback.print_exc()
                    console.print("\n[red]âŒ å¢å¼ºç‰ˆå¿…é¡»ä½¿ç”¨ vLLMï¼Œç¨‹åºé€€å‡º[/red]")
                    sys.exit(1)
                except Exception as e:
                    console.print(f"[red]âŒ vLLM åˆå§‹åŒ–å¤±è´¥: æœªçŸ¥é”™è¯¯[/red]")
                    console.print(f"[red]é”™è¯¯ç±»å‹: {type(e).__name__}[/red]")
                    console.print(f"[red]é”™è¯¯è¯¦æƒ…: {e}[/red]")
                    
                    import traceback
                    traceback.print_exc()
                    console.print("\n[red]âŒ å¢å¼ºç‰ˆå¿…é¡»ä½¿ç”¨ vLLMï¼Œç¨‹åºé€€å‡º[/red]")
                    sys.exit(1)
        
        # ç°åœ¨ vLLM å·²ç»åˆå§‹åŒ–å®Œæˆï¼Œå¯ä»¥å®‰å…¨åœ°åˆå§‹åŒ–ä¼šä½¿ç”¨ CUDA çš„ç»„ä»¶
        # åˆå§‹åŒ– RAGEnhancedAgentMemoryï¼ˆä¼šåŠ è½½ embedding å’Œ reranker æ¨¡å‹ï¼Œä½¿ç”¨ CUDAï¼‰
        # å»¶è¿Ÿåˆå§‹åŒ–å¯ä»¥é¿å… vLLM å­è¿›ç¨‹ä¸­çš„ duplicate template name é”™è¯¯
        console.print("[dim]åˆå§‹åŒ– RAG å¢å¼ºå‹ Agent è®°å¿†ç³»ç»Ÿ...[/dim]")
        self.agent = RAGEnhancedAgentMemory(session_id=self.session_id)
        
        # æ€§èƒ½è®°å½•
        self.latencies = []
        self.ttfts = []  # é¦–å­—å»¶è¿Ÿ
        self.tokens_per_second = []  # æ¯ç§’ tokensï¼ˆååé‡ï¼‰
        self.tokens_generated = []  # æ¯æ¬¡ç”Ÿæˆçš„ tokens æ•°
        self.memory_snapshots = []
        
        # é•¿æœŸè®°å¿†å¬å›æµ‹è¯•è®°å½•
        self.recall_test_cases = {
            "å·¥å·": {"query": "æˆ‘çš„å·¥å·", "expected": "9527", "found": False, "retrieved_content": ""},
            "å’–å•¡ä¹ æƒ¯": {"query": "æˆ‘å–å’–å•¡çš„ä¹ æƒ¯", "expected": "å†°ç¾å¼ï¼Œä¸åŠ ç³–", "found": False, "retrieved_content": ""},
            "è®¢å•å·": {"query": "æŸ¥è¯¢çš„è®¢å•å·", "expected": "ORDER_2026_001", "found": False, "retrieved_content": ""},
        }
    
    def generate_script(self) -> List[str]:
        """ç”Ÿæˆ 100 è½®æ¨¡æ‹Ÿå‰§æœ¬"""
        script = []
        
        # Phase 1: è®¾å®šäººè®¾ (1-5è½®) - æµ‹è¯•è®°å¿†å†™å…¥
        script.extend([
            "ä½ å¥½ï¼Œæˆ‘æ˜¯æµ‹è¯•å‘˜é˜¿å¼ºã€‚æˆ‘çš„å·¥å·æ˜¯ 9527ã€‚",  # å…³é”®ä¿¡æ¯ç‚¹ 1
            "æˆ‘å–œæ¬¢å–å†°ç¾å¼ï¼Œä¸åŠ ç³–ã€‚",              # å…³é”®ä¿¡æ¯ç‚¹ 2
            "æˆ‘ä»¬ä»Šå¤©çš„ä»»åŠ¡æ˜¯æµ‹è¯• Agent çš„æé™æ€§èƒ½ã€‚",
            "ä½ åªéœ€è¦ç®€çŸ­å›å¤æ”¶åˆ°å³å¯ã€‚",
            "å‡†å¤‡å¥½äº†å—ï¼Ÿ"
        ])
        
        # Phase 2: ç–¯ç‹‚å¤è¯» (6-35è½®) - æµ‹è¯•è¯­ä¹‰å»é‡ & vLLM ç¼“å­˜
        # æ¨¡æ‹Ÿç”¨æˆ·ä¸æ–­æŸ¥è¯¢åŒä¸€ä»¶äº‹ï¼ŒéªŒè¯å­˜å‚¨æ˜¯å¦ä¼šçˆ†ç‚¸
        # ä½¿ç”¨ ORDER_2026_001 ä»¥åŒ¹é…å½“å‰å¹´ä»½
        for _ in range(10):
            script.append("å¸®æˆ‘æŸ¥ä¸€ä¸‹è®¢å• ORDER_2026_001 çš„çŠ¶æ€")
            script.append("è®¢å• ORDER_2026_001 å‘è´§äº†å—ï¼Ÿ")
            script.append("è¿˜æ²¡æŸ¥åˆ° ORDER_2026_001 å—ï¼Ÿ")
            
        # Phase 3: ä½ä»·å€¼çŒæ°´ (36-85è½®) - æµ‹è¯•ä½ä»·å€¼è¿‡æ»¤
        # æ¨¡æ‹Ÿå¤§é‡æ— æ„ä¹‰å¯’æš„
        fillers = ["å¥½çš„", "æ”¶åˆ°", "æ˜ç™½", "OK", "è°¢è°¢", "åœ¨å—", "å—¯å—¯", "ä¸é”™", "å“ˆå“ˆ", "ç¡®å®"]
        for i in range(50):
            script.append(fillers[i % len(fillers)])
            
        # Phase 4: è®°å¿†çªå‡»æ£€æŸ¥ (86-100è½®) - æµ‹è¯•é•¿ç¨‹å¬å›
        script.extend([
            "æµ‹è¯•å¿«ç»“æŸäº†ï¼Œç¨å¾®æ€»ç»“ä¸€ä¸‹ã€‚",
            "å¯¹äº†ï¼Œè¿˜è®°å¾—æˆ‘æ˜¯è°å—ï¼ŸæŠ¥ä¸Šæˆ‘çš„å·¥å·ã€‚",  # æ£€æŸ¥ç‚¹ 1
            "æˆ‘åˆšæ‰è¯´æˆ‘å–å’–å•¡æœ‰ä»€ä¹ˆä¹ æƒ¯ï¼Ÿ",        # æ£€æŸ¥ç‚¹ 2
            "æˆ‘ä»¬åˆšæ‰æŸ¥è¯¢äº†å“ªä¸ªè®¢å•å·ï¼Ÿ",          # æ£€æŸ¥ç‚¹ 3
            "è¿™ä¸€ç™¾å¥èŠä¸‹æ¥ï¼Œä½ ç´¯ä¸ç´¯ï¼Ÿ",
            "å†è§ï¼"
        ])
        
        return script
    
    def call_llm(self, user_input: str, context: List[str]) -> tuple[str, float]:
        """
        è°ƒç”¨æœ¬åœ°æ¨ç†å¼•æ“ç”Ÿæˆå›å¤
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
            
        Returns:
            (ç”Ÿæˆçš„å›å¤, æ€»å»¶è¿Ÿæ—¶é—´(ms))
        """
        if not self.use_llm or not self.inference_engine:
            console.print("[red]âŒ æ¨ç†å¼•æ“ä¸å¯ç”¨ï¼Œç¨‹åºé€€å‡º[/red]")
            sys.exit(1)
        
        # æ„å»ºæç¤ºè¯
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç®€æ´ã€å‡†ç¡®åœ°å›å¤ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_text = ""
        if context:
            context_text = "\n".join([f"- {doc}" for doc in context[:5]])  # æœ€å¤šä½¿ç”¨5æ¡ä¸Šä¸‹æ–‡
            context_text = f"ç›¸å…³ä¸Šä¸‹æ–‡ï¼š\n{context_text}\n\n"
        
        user_prompt = f"{context_text}ç”¨æˆ·é—®é¢˜ï¼š{user_input}"
        
        try:
            # è°ƒç”¨æœ¬åœ°æ¨ç†å¼•æ“
            response, metrics = self.inference_engine.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.7,
                max_tokens=512  # max_model_len=1024ï¼Œå¯ä»¥è®¾ç½®æ›´å¤§çš„å€¼
            )
            
            # æå–å»¶è¿Ÿä¿¡æ¯
            total_time = metrics.get("total_time", 0) * 1000  # è½¬æ¢ä¸º ms
            ttft = metrics.get("ttft", 0) * 1000  # è½¬æ¢ä¸º ms
            tps = metrics.get("tokens_per_second", 0)  # æ¯ç§’ tokens
            tokens = metrics.get("tokens_generated", 0)  # ç”Ÿæˆçš„ tokens æ•°
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            if ttft > 0:
                self.ttfts.append(ttft)
            if tps > 0:
                self.tokens_per_second.append(tps)
            if tokens > 0:
                self.tokens_generated.append(tokens)
            
            return response, total_time
            
        except Exception as e:
            console.print(f"[red]âŒ æœ¬åœ°æ¨ç†å¼•æ“è°ƒç”¨å¤±è´¥: {e}[/red]")
            import traceback
            traceback.print_exc()
            console.print("\n[red]âŒ æ¨ç†å¼•æ“è°ƒç”¨å¤±è´¥ï¼Œç¨‹åºé€€å‡º[/red]")
            sys.exit(1)
    
    def _test_recall(self, user_input: str, context_docs: List[str], response: str):
        """
        æµ‹è¯•é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context_docs: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£
            response: Agent å›å¤
        """
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ç”¨äºæ£€æŸ¥
        all_text = " ".join(context_docs) + " " + response
        
        # æ£€æŸ¥å·¥å·å¬å›ï¼ˆæµ‹è¯•ç‚¹ï¼šç¬¬87è½® "è¿˜è®°å¾—æˆ‘æ˜¯è°å—ï¼ŸæŠ¥ä¸Šæˆ‘çš„å·¥å·ã€‚"ï¼‰
        if ("å·¥å·" in user_input or "æˆ‘æ˜¯è°" in user_input) and not self.recall_test_cases["å·¥å·"]["found"]:
            # æ£€æŸ¥æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å’Œå›å¤ä¸­æ˜¯å¦åŒ…å«å·¥å·
            if "9527" in all_text:
                self.recall_test_cases["å·¥å·"]["found"] = True
                # æ‰¾åˆ°åŒ…å«å·¥å·çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ
                for doc in context_docs:
                    if "9527" in doc:
                        self.recall_test_cases["å·¥å·"]["retrieved_content"] = doc[:150]
                        break
                if not self.recall_test_cases["å·¥å·"]["retrieved_content"]:
                    self.recall_test_cases["å·¥å·"]["retrieved_content"] = response[:150]
        
        # æ£€æŸ¥å’–å•¡ä¹ æƒ¯å¬å›ï¼ˆæµ‹è¯•ç‚¹ï¼šç¬¬88è½® "æˆ‘åˆšæ‰è¯´æˆ‘å–å’–å•¡æœ‰ä»€ä¹ˆä¹ æƒ¯ï¼Ÿ"ï¼‰
        if ("å’–å•¡" in user_input or ("å–" in user_input and "ä¹ æƒ¯" in user_input)) and not self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["found"]:
            if "å†°ç¾å¼" in all_text or "ä¸åŠ ç³–" in all_text:
                self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["found"] = True
                # æ‰¾åˆ°åŒ…å«å’–å•¡ä¹ æƒ¯çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ
                for doc in context_docs:
                    if "å†°ç¾å¼" in doc or "ä¸åŠ ç³–" in doc:
                        self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["retrieved_content"] = doc[:150]
                        break
                if not self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["retrieved_content"]:
                    self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["retrieved_content"] = response[:150]
        
        # æ£€æŸ¥è®¢å•å·å¬å›ï¼ˆæµ‹è¯•ç‚¹ï¼šç¬¬89è½® "æˆ‘ä»¬åˆšæ‰æŸ¥è¯¢äº†å“ªä¸ªè®¢å•å·ï¼Ÿ"ï¼‰
        if ("è®¢å•" in user_input and "å“ªä¸ª" in user_input) and not self.recall_test_cases["è®¢å•å·"]["found"]:
            if "ORDER_2026_001" in all_text or "ORDER_2024_001" in all_text:
                self.recall_test_cases["è®¢å•å·"]["found"] = True
                # æ‰¾åˆ°åŒ…å«è®¢å•å·çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ
                for doc in context_docs:
                    if "ORDER_2026_001" in doc or "ORDER_2024_001" in doc:
                        self.recall_test_cases["è®¢å•å·"]["retrieved_content"] = doc[:150]
                        break
                if not self.recall_test_cases["è®¢å•å·"]["retrieved_content"]:
                    self.recall_test_cases["è®¢å•å·"]["retrieved_content"] = response[:150]
        

    def run(self):
        script = self.generate_script()
        console.print(f"[bold green]ğŸš€ Auto-Player å¯åŠ¨ï¼ç›®æ ‡: {len(script)} è½®å¯¹è¯[/bold green]")
        console.print(f"Session ID: {self.session_id}")
        # æ£€æŸ¥é…ç½®
        try:
            enable_filter = getattr(self.settings, 'enable_low_value_filter', True)
        except:
            enable_filter = True
        
        if not self.use_llm:
            console.print("[red]âŒ æ¨ç†å¼•æ“æœªåˆå§‹åŒ–ï¼Œç¨‹åºé€€å‡º[/red]")
            sys.exit(1)
        
        llm_status = "âœ… vLLM" if (self.use_llm and not self.use_baseline) else ("âœ… Baseline" if (self.use_llm and self.use_baseline) else "âŒ æœªåˆå§‹åŒ–")
        console.print(f"é…ç½®æ£€æŸ¥: å­˜å‚¨ä¼˜åŒ–={'âœ…' if enable_filter else 'âŒ'}, LLM={llm_status}, æ¨¡å‹={self.model_path}")
        print("-" * 60)

        # è·å–åˆå§‹å­˜å‚¨çŠ¶æ€
        mem_stats_start = self.agent.long_term_memory.get_stats()
        start_count = mem_stats_start.get("total_memories", 0)

        for i, user_input in enumerate(track(script, description="æ­£åœ¨äº¤äº’...")):
            start_time = time.time()
            
            # æ£€ç´¢ä¸Šä¸‹æ–‡ï¼ˆæ¨¡æ‹Ÿ Agent æ£€ç´¢è¿‡ç¨‹ï¼‰
            from src.graph.state import AgentState
            state: AgentState = {
                "input": user_input,
                "chat_history": [],
                "documents": [],
                "document_metadata": [],
                "generation": "",
                "relevance_score": "",
                "hallucination_score": "",
                "retry_count": 0,
            }
            
            # æ£€ç´¢ä¸Šä¸‹æ–‡
            context_result = self.agent.retrieve_context(state)
            context_docs = context_result.get('documents', [])
            
            # è°ƒç”¨ LLM ç”Ÿæˆå›å¤
            response, llm_latency = self.call_llm(user_input, context_docs)
            
            # ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡
            self.agent.save_context(
                inputs={"input": user_input},
                outputs={"generation": response}
            )
            
            end_time = time.time()
            total_latency = (end_time - start_time) * 1000  # ms
            
            self.latencies.append(total_latency)
            
            # æµ‹è¯•é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›ï¼ˆPhase 4: è®°å¿†çªå‡»æ£€æŸ¥ï¼‰
            if i >= 85:  # Phase 4 çš„æ£€æŸ¥ç‚¹
                self._test_recall(user_input, context_docs, response)
            
            # ç®€å•æ‰“å°äº¤äº’ï¼ˆæ¯10è½®æˆ–å…³é”®èŠ‚ç‚¹ï¼‰
            if i < 5 or i > 85 or i % 20 == 0:
                console.print(f"[blue]User({i+1}):[/blue] {user_input}")
                ttft_info = f", TTFT: {self.ttfts[-1]:.1f}ms" if self.ttfts else ""
                console.print(f"[yellow]Agent:[/yellow] {response[:80]}... [dim](æ€»å»¶è¿Ÿ: {total_latency:.1f}ms{ttft_info}, æ£€ç´¢åˆ°{len(context_docs)}æ¡)[/dim]")

        # è·å–æœ€ç»ˆå­˜å‚¨çŠ¶æ€
        mem_stats_end = self.agent.long_term_memory.get_stats()
        end_count = mem_stats_end.get("total_memories", 0)
        
        # è®¡ç®—ä¿¡æ¯ä¿ç•™ç‡
        # æœ‰æ•ˆä¿¡æ¯ï¼šPhase 1 (5è½®) + Phase 2ä¸­çš„è®¢å•ä¿¡æ¯ï¼ˆè™½ç„¶é‡å¤ï¼Œä½†ä¿¡æ¯æœ‰ä»·å€¼ï¼‰
        # ç†è®ºä¸Šåº”è¯¥ä¿ç•™çš„å…³é”®ä¿¡æ¯ï¼šå·¥å·ã€å’–å•¡ä¹ æƒ¯ã€è®¢å•å·
        # å®é™…å­˜å‚¨ï¼šend_count - start_count
        # ä¿¡æ¯ä¿ç•™ç‡ = å®é™…å­˜å‚¨çš„å…³é”®ä¿¡æ¯ / åº”è¯¥ä¿ç•™çš„å…³é”®ä¿¡æ¯
        # ç®€åŒ–è®¡ç®—ï¼šå®é™…å­˜å‚¨æ•° / æ€»è½®æ•°ï¼ˆè€ƒè™‘å»é‡å’Œè¿‡æ»¤åï¼‰
        total_turns = len(script)
        actual_stored = end_count - start_count
        
        # ä¿¡æ¯ä¿ç•™ç‡ï¼šå®é™…å­˜å‚¨çš„å…³é”®ä¿¡æ¯æ¯”ä¾‹
        # ç†è®ºä¸Šï¼Œ100è½®å¯¹è¯ä¸­ï¼Œæœ‰æ•ˆä¿¡æ¯çº¦20è½®ï¼ˆ5è½®äººè®¾ + 15è½®è®¢å•ç›¸å…³ï¼‰
        # ä½†ç»è¿‡å»é‡å’Œè¿‡æ»¤åï¼Œå®é™…å­˜å‚¨åº”è¯¥æ›´å°‘
        # ä¿¡æ¯ä¿ç•™ç‡ = å®é™…å­˜å‚¨æ•° / ç†è®ºæœ‰æ•ˆä¿¡æ¯æ•°
        theoretical_effective = 20  # ä¼°ç®—çš„æœ‰æ•ˆä¿¡æ¯æ•°
        retention_rate = (actual_stored / theoretical_effective * 100) if theoretical_effective > 0 else 0
        
        # å¦‚æœå®é™…å­˜å‚¨æ•°è¶…è¿‡ç†è®ºå€¼ï¼Œè¯´æ˜å¯èƒ½æ²¡æœ‰å……åˆ†å»é‡
        if actual_stored > theoretical_effective:
            retention_rate = 100.0  # è‡³å°‘ä¿ç•™äº†æ‰€æœ‰æœ‰æ•ˆä¿¡æ¯
        
        self.generate_report(len(script), start_count, end_count, retention_rate)

    def generate_report(self, total_turns, start_count, end_count, retention_rate):
        console.print("\n\n")
        console.rule("[bold red]ğŸ“Š æ’ä»¶æ•ˆæœéªŒè¯æŠ¥å‘Š[/bold red]")
        
        # 1. æ€§èƒ½æŒ‡æ ‡
        avg_latency = statistics.mean(self.latencies)
        p95_latency = statistics.quantiles(self.latencies, n=20)[18]  # 95th percentile
        
        perf_table = Table(title="âš¡ æ¨ç†æ€§èƒ½ (Inference Speed)")
        perf_table.add_column("æŒ‡æ ‡", style="cyan")
        perf_table.add_column("ç»“æœ", style="green")
        perf_table.add_column("è¯„ä»·")
        
        # è®¡ç®—é¦–å­—å»¶è¿Ÿç»Ÿè®¡
        avg_ttft = statistics.mean(self.ttfts) if self.ttfts else 0
        p95_ttft = statistics.quantiles(self.ttfts, n=20)[18] if len(self.ttfts) >= 20 else 0
        
        perf_table.add_row("å¹³å‡å»¶è¿Ÿ", f"{avg_latency:.1f} ms", "ğŸš€ ç§’å›" if avg_latency < 500 else "âš ï¸ æ­£å¸¸")
        perf_table.add_row("P95 å»¶è¿Ÿ", f"{p95_latency:.1f} ms", "åœ¨é«˜è´Ÿè½½ä¸‹è¡¨ç°ç¨³å®š")
        if avg_ttft > 0:
            perf_table.add_row("å¹³å‡é¦–å­—å»¶è¿Ÿ (TTFT)", f"{avg_ttft:.1f} ms", "ğŸš€ æå¿«" if avg_ttft < 200 else "âœ… è‰¯å¥½")
            perf_table.add_row("P95 é¦–å­—å»¶è¿Ÿ", f"{p95_ttft:.1f} ms", "é«˜è´Ÿè½½ä¸‹é¦–å­—å“åº”ç¨³å®š")
        
        # ååé‡æŒ‡æ ‡
        avg_tps = statistics.mean(self.tokens_per_second) if self.tokens_per_second else 0
        p95_tps = statistics.quantiles(self.tokens_per_second, n=20)[18] if len(self.tokens_per_second) >= 20 else 0
        
        if avg_tps > 0:
            perf_table.add_row("å¹³å‡ååé‡", f"{avg_tps:.1f} tokens/s", "ğŸš€ ä¼˜ç§€" if avg_tps > 50 else "âœ… è‰¯å¥½")
            perf_table.add_row("P95 ååé‡", f"{p95_tps:.1f} tokens/s", "é«˜è´Ÿè½½ä¸‹ååé‡ç¨³å®š")
        
        perf_table.add_row("æ€»è€—æ—¶", f"{sum(self.latencies)/1000:.1f} s", f"å¤„ç† {total_turns} è½®å¯¹è¯")
        
        console.print(perf_table)
        console.print("\n")

        # 2. å­˜å‚¨æŒ‡æ ‡
        actual_stored = end_count - start_count
        # ç†è®ºä¸Šå¦‚æœä¸ä¼˜åŒ–ï¼Œ100è½®å¯¹è¯è‡³å°‘ä¼šå­˜100æ¡ï¼ˆæˆ–50æ¡ï¼Œçœ‹å®ç°ï¼‰
        # æˆ‘ä»¬çš„è„šæœ¬é‡Œæœ‰30æ¡å¤è¯»+50æ¡çŒæ°´ï¼Œç†è®ºæœ‰æ•ˆä¿¡æ¯åªæœ‰çº¦20æ¡
        reduction_rate = (1 - (actual_stored / total_turns)) * 100
        
        store_table = Table(title="ğŸ’¾ å­˜å‚¨ä¼˜åŒ– (Storage Efficiency)")
        store_table.add_column("æŒ‡æ ‡", style="cyan")
        store_table.add_column("æ•°æ®", style="magenta")
        store_table.add_column("è¯´æ˜")
        
        store_table.add_row("è¾“å…¥å¯¹è¯æ€»æ•°", str(total_turns), "æ¨¡æ‹Ÿçš„ç”¨æˆ·è¾“å…¥")
        store_table.add_row("æ•°æ®åº“ä¸­å­˜å‚¨æ•°", str(actual_stored), "æ•°æ®åº“ä¸­å¢åŠ çš„å‘é‡æ•°")
        store_table.add_row("å™ªéŸ³è¿‡æ»¤ç‡", f"{reduction_rate:.1f}%", "è¢«å»é‡/è¿‡æ»¤æ‰çš„åºŸè¯æ¯”ä¾‹")
        store_table.add_row("ä¿¡æ¯ä¿ç•™ç‡", f"{retention_rate:.1f}%", "å…³é”®ä¿¡æ¯ä¿ç•™æ¯”ä¾‹")
        
        console.print(store_table)
        console.print("\n")
        
        # 3. é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›
        recall_table = Table(title="ğŸ§  é•¿æœŸè®°å¿†å¬å›èƒ½åŠ› (Long-Term Memory Recall)")
        recall_table.add_column("æµ‹è¯•é¡¹", style="cyan", width=12)
        recall_table.add_column("æœŸæœ›ä¿¡æ¯", style="yellow", width=20)
        recall_table.add_column("å¬å›çŠ¶æ€", style="green", width=12)
        recall_table.add_column("æ£€ç´¢å†…å®¹é¢„è§ˆ", style="dim", width=40)
        
        recall_success_count = 0
        for test_name, test_case in self.recall_test_cases.items():
            status = "âœ… æˆåŠŸ" if test_case["found"] else "âŒ å¤±è´¥"
            status_style = "green" if test_case["found"] else "red"
            if test_case["found"]:
                recall_success_count += 1
            
            retrieved_preview = test_case["retrieved_content"][:50] + "..." if test_case["retrieved_content"] else "[dim]æœªæ£€ç´¢åˆ°[/dim]"
            
            recall_table.add_row(
                test_name,
                test_case["expected"],
                f"[{status_style}]{status}[/{status_style}]",
                retrieved_preview
            )
        
        recall_rate = (recall_success_count / len(self.recall_test_cases)) * 100 if self.recall_test_cases else 0
        
        console.print(recall_table)
        
        # å¬å›èƒ½åŠ›è¯¦ç»†è¯´æ˜
        console.print(f"\n[bold cyan]ğŸ“Š å¬å›èƒ½åŠ›åˆ†æï¼š[/bold cyan]")
        console.print(f"  â€¢ æµ‹è¯•åœºæ™¯ï¼šåœ¨ 100 è½®å¯¹è¯åï¼Œæµ‹è¯•ç³»ç»Ÿæ˜¯å¦èƒ½ä»é•¿æœŸè®°å¿†ä¸­å¬å›æ—©æœŸï¼ˆç¬¬1-5è½®ï¼‰çš„å…³é”®ä¿¡æ¯")
        console.print(f"  â€¢ å¬å›æˆåŠŸç‡ï¼š{recall_rate:.1f}% ({recall_success_count}/{len(self.recall_test_cases)})")
        console.print(f"  â€¢ æµ‹è¯•ä¿¡æ¯ç‚¹ï¼šå·¥å·ï¼ˆç¬¬1è½®ï¼‰ã€å’–å•¡ä¹ æƒ¯ï¼ˆç¬¬2è½®ï¼‰ã€è®¢å•å·ï¼ˆç¬¬6-35è½®ï¼‰")
        console.print()
        
        # 4. ç»“è®º
        console.rule("[bold]ğŸ“ æœ€ç»ˆç»“è®º[/bold]")
        if not self.use_llm:
            console.print("[red]âŒ æ¨ç†å¼•æ“æœªåˆå§‹åŒ–ï¼Œç¨‹åºé€€å‡º[/red]")
            sys.exit(1)
        
        if not self.use_baseline:
            # vLLM æ¨¡å¼
            if avg_latency < 2000:
                console.print("âœ… [bold green]vLLM æ¨ç†æ­£å¸¸ï¼š[/bold green] å“åº”é€Ÿåº¦è‰¯å¥½ã€‚")
            else:
                console.print("âš ï¸ [bold yellow]vLLM å»¶è¿Ÿè¾ƒé«˜ï¼š[/bold yellow] è¯·æ£€æŸ¥ GPU é…ç½®æˆ–æ¨¡å‹è·¯å¾„ã€‚")
            
            if avg_ttft > 0 and avg_ttft < 500:
                console.print(f"âœ… [bold green]é¦–å­—å»¶è¿Ÿä¼˜ç§€ï¼š[/bold green] å¹³å‡ TTFT {avg_ttft:.1f}msï¼ŒvLLM ä¼˜åŒ–ç”Ÿæ•ˆã€‚")
            elif avg_ttft > 0:
                console.print(f"âš ï¸ [bold yellow]é¦–å­—å»¶è¿Ÿï¼š[/bold yellow] å¹³å‡ TTFT {avg_ttft:.1f}msï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–é…ç½®ã€‚")
        else:
            # Baseline æ¨¡å¼
            if avg_latency < 2000:
                console.print("âœ… [bold green]Baseline æ¨ç†æ­£å¸¸ï¼š[/bold green] å“åº”é€Ÿåº¦è‰¯å¥½ã€‚")
            else:
                console.print("âš ï¸ [bold yellow]Baseline å»¶è¿Ÿè¾ƒé«˜ï¼š[/bold yellow] è¯·æ£€æŸ¥ GPU é…ç½®æˆ–æ¨¡å‹è·¯å¾„ã€‚")
            
        if reduction_rate > 70:
            console.print("âœ… [bold green]å­˜å‚¨ä¼˜åŒ–ç”Ÿæ•ˆï¼š[/bold green] æˆåŠŸè¿‡æ»¤äº†ç»å¤§å¤šæ•°é‡å¤å’Œæ— æ•ˆä¿¡æ¯ã€‚")
        else:
            console.print("âš ï¸ [bold red]å­˜å‚¨ä¼˜åŒ–æœªç”Ÿæ•ˆï¼š[/bold red] æ•°æ®åº“å­˜å…¥äº†è¿‡å¤šå†—ä½™ä¿¡æ¯ã€‚")
        
        # é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›è¯„ä»·
        if recall_rate >= 100:
            console.print(f"âœ… [bold green]é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›ä¼˜ç§€ï¼š[/bold green] å¬å›æˆåŠŸç‡ {recall_rate:.1f}%ï¼Œæ‰€æœ‰å…³é”®ä¿¡æ¯éƒ½èƒ½æ­£ç¡®å¬å›ã€‚")
        elif recall_rate >= 66:
            console.print(f"âœ… [bold green]é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›è‰¯å¥½ï¼š[/bold green] å¬å›æˆåŠŸç‡ {recall_rate:.1f}%ï¼Œå¤§éƒ¨åˆ†å…³é”®ä¿¡æ¯èƒ½æ­£ç¡®å¬å›ã€‚")
        elif recall_rate > 0:
            console.print(f"âš ï¸ [bold yellow]é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›ä¸€èˆ¬ï¼š[/bold yellow] å¬å›æˆåŠŸç‡ {recall_rate:.1f}%ï¼Œéƒ¨åˆ†å…³é”®ä¿¡æ¯æœªèƒ½å¬å›ï¼Œå»ºè®®æ£€æŸ¥æ£€ç´¢é…ç½®ã€‚")
        else:
            console.print(f"âŒ [bold red]é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›ä¸è¶³ï¼š[/bold red] å¬å›æˆåŠŸç‡ 0%ï¼Œå…³é”®ä¿¡æ¯æœªèƒ½å¬å›ï¼Œè¯·æ£€æŸ¥é•¿æœŸè®°å¿†å­˜å‚¨å’Œæ£€ç´¢é…ç½®ã€‚")

if __name__ == "__main__":
    import argparse
    import atexit
    import signal
    
    def cleanup_resources():
        """æ¸…ç†èµ„æºï¼Œé¿å…é€€å‡ºæ—¶çš„è­¦å‘Š"""
        try:
            if player is not None and hasattr(player, 'agent'):
                # ä½¿ç”¨ RAGEnhancedAgentMemory çš„ close æ–¹æ³•
                if hasattr(player.agent, 'close'):
                    player.agent.close()
        except Exception:
            pass  # å¿½ç•¥æ‰€æœ‰æ¸…ç†é”™è¯¯
    
    parser = argparse.ArgumentParser(description="Auto-Player: å…¨è‡ªåŠ¨ Agent äº¤äº’éªŒè¯è„šæœ¬")
    parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ VLLM_MODEL_PATH è®¾ç½®ï¼Œé»˜è®¤: Qwen/Qwen2.5-7B-Instructï¼‰"
    )
    parser.add_argument(
        "--use-baseline",
        action="store_true",
        help="ä½¿ç”¨åŸºçº¿æ¨ç†å¼•æ“ï¼ˆTransformersï¼‰è€Œä¸æ˜¯ vLLMï¼ˆç”¨äºå¯¹æ¯”æµ‹è¯•ï¼‰"
    )
    
    args = parser.parse_args()
    
    player = None
    try:
        player = AutoPlayer(
            model_path=args.model_path,
            use_baseline=args.use_baseline
        )
        
        # æ³¨å†Œæ¸…ç†å‡½æ•°
        atexit.register(cleanup_resources)
        
        # æ³¨å†Œä¿¡å·å¤„ç†ï¼ˆç”¨äº Ctrl+C ç­‰ï¼‰
        def signal_handler(sig, frame):
            cleanup_resources()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        player.run()
    except KeyboardInterrupt:
        console.print("\n[yellow]âš ï¸  ç”¨æˆ·ä¸­æ–­[/yellow]")
        cleanup_resources()
        sys.exit(0)
    except Exception as e:
        console.print(f"[red]âŒ è¿è¡Œå‡ºé”™: {e}[/red]")
        cleanup_resources()
        sys.exit(1)
    finally:
        # ç¡®ä¿èµ„æºæ¸…ç†
        cleanup_resources()