"""
Baseline Auto-Player: åŸºçº¿å¯¹ç…§ç»„è„šæœ¬
ç”¨äºå¯¹æ¯”æµ‹è¯•ï¼šä¸ä½¿ç”¨ RAGEnhancedAgentMemory æ’ä»¶ï¼Œå…¶ä»–æ¡ä»¶ç›¸åŒ

å¯¹æ¯”ç»´åº¦ï¼š
1. æ¨ç†æ€§èƒ½ï¼ˆå»¶è¿Ÿã€TTFTï¼‰
2. å­˜å‚¨æ•ˆç‡ï¼ˆæ— ä¼˜åŒ–ï¼Œæ‰€æœ‰å¯¹è¯éƒ½å­˜å‚¨ï¼‰
3. é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›ï¼ˆæ— é•¿æœŸè®°å¿†ï¼Œæ— æ³•å¬å›æ—©æœŸä¿¡æ¯ï¼‰
"""

import sys
import time
import uuid
import statistics
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
from collections import deque
from rich.console import Console
from rich.table import Table
from rich.progress import track

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åœ¨å¯¼å…¥ä»»ä½•åº“ä¹‹å‰è®¾ç½® HuggingFace é•œåƒæºï¼ˆå¦‚æœé…ç½®äº†ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv(project_root / ".env")
    hf_endpoint = os.getenv("HF_ENDPOINT")
    if hf_endpoint:
        os.environ["HF_ENDPOINT"] = hf_endpoint
        try:
            from huggingface_hub import set_endpoint
            set_endpoint(hf_endpoint)
        except ImportError:
            pass
except Exception:
    pass

# å¯¼å…¥æœ¬åœ°æ¨ç†å¼•æ“ï¼ˆåŸºçº¿ç‰ˆä½¿ç”¨ BaselineInferenceï¼‰
try:
    from src.inference import BaselineInference, TRANSFORMERS_AVAILABLE
    from src.config import get_settings
    LOCAL_INFERENCE_AVAILABLE = TRANSFORMERS_AVAILABLE
except ImportError as e:
    LOCAL_INFERENCE_AVAILABLE = False
    BaselineInference = None
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥åŸºçº¿æ¨ç†å¼•æ“: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£… transformers: pip install transformers torch")

# åˆå§‹åŒ– Rich æ§åˆ¶å°
console = Console()


class BaselineAgent:
    """
    åŸºçº¿ Agentï¼ˆä¸ä½¿ç”¨ RAGEnhancedAgentMemoryï¼‰
    
    ä½¿ç”¨ç®€å•çš„æ»‘åŠ¨çª—å£è®°å¿†ï¼Œåªä¿ç•™æœ€è¿‘ N è½®å¯¹è¯
    ä¸è¿›è¡Œè¯­ä¹‰æ£€ç´¢ã€å»é‡ã€è¿‡æ»¤ç­‰ä¼˜åŒ–
    """
    
    def __init__(self, window_size: int = 10):
        """
        åˆå§‹åŒ–åŸºçº¿ Agent
        
        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆä¿ç•™çš„å¯¹è¯è½®æ•°ï¼‰
        """
        self.window_size = window_size
        self.conversation_history = deque(maxlen=window_size)
        self.total_turns = 0
        self.session_id = f"baseline_{uuid.uuid4().hex[:8]}"
        
    def add_conversation(self, human: str, ai: str) -> None:
        """æ·»åŠ å¯¹è¯åˆ°å†å²ï¼ˆæ»‘åŠ¨çª—å£ï¼‰"""
        self.total_turns += 1
        self.conversation_history.append({
            "human": human,
            "ai": ai,
            "turn": self.total_turns
        })
    
    def get_context(self, query: str) -> List[str]:
        """
        è·å–å½“å‰ä¸Šä¸‹æ–‡ï¼ˆåªåŒ…å«çª—å£å†…çš„å¯¹è¯ï¼‰
        
        Args:
            query: å½“å‰æŸ¥è¯¢
            
        Returns:
            ä¸Šä¸‹æ–‡åˆ—è¡¨
        """
        context = []
        for turn in self.conversation_history:
            context.append(f"ç”¨æˆ·: {turn['human']}\nåŠ©æ‰‹: {turn['ai']}")
        return context
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_turns": self.total_turns,
            "window_size": self.window_size,
            "current_history_size": len(self.conversation_history),
            "has_long_term_memory": False,
        }


class BaselineAutoPlayer:
    def __init__(self, model_path: Optional[str] = None, window_size: int = 10):
        """
        åˆå§‹åŒ–åŸºçº¿ Auto-Player
        
        åŸºçº¿ç³»ç»Ÿä½¿ç”¨ç®€å•çš„æ»‘åŠ¨çª—å£è®°å¿†ï¼ˆä¸ä½¿ç”¨ RAGEnhancedAgentMemoryï¼‰ï¼Œ
        å”¯ä¸€çš„åŒºåˆ«æ˜¯ï¼š
        - å¢å¼ºç‰ˆï¼šä½¿ç”¨ RAGEnhancedAgentMemory + VLLMInference
        - åŸºçº¿ç‰ˆï¼šä½¿ç”¨æ»‘åŠ¨çª—å£è®°å¿† + BaselineInference
        
        ä½†ä¸¤è€…ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ï¼ˆéƒ½æ˜¯ç”¨æˆ·é€‰æ‹©çš„ï¼‰
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„æˆ–åç§°ï¼ˆå¦‚æœä¸º Noneï¼Œä»é…ç½®è¯»å–ï¼‰
            window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆä¿ç•™çš„å¯¹è¯è½®æ•°ï¼‰
        """
        self.settings = get_settings()
        self.session_id = f"baseline_{uuid.uuid4().hex[:8]}"
        self.agent = BaselineAgent(window_size=window_size)
        
        # åˆå§‹åŒ–æœ¬åœ°æ¨ç†å¼•æ“ï¼ˆåŸºçº¿ç‰ˆä½¿ç”¨ BaselineInferenceï¼‰
        # åŸºçº¿ç³»ç»Ÿå¼ºåˆ¶ä½¿ç”¨æœ¬åœ° transformers æ¨¡å‹ï¼ˆHuggingFace æ¨¡å‹æˆ–æœ¬åœ°è·¯å¾„ï¼‰ï¼Œä¸ä½¿ç”¨ API
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ model_pathï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        if model_path:
            self.model_path = model_path
        else:
            # ä¼˜å…ˆä½¿ç”¨æ–°çš„é…ç½®é¡¹ï¼Œå…¶æ¬¡ä½¿ç”¨å·²åºŸå¼ƒçš„é…ç½®é¡¹
            self.model_path = self.settings.vllm_model or self.settings.vllm_model_path
        
        # åŸºçº¿ç³»ç»Ÿå¿…é¡»ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä¸èƒ½ä½¿ç”¨ API
        # å¦‚æœä»ç„¶æ²¡æœ‰æ¨¡å‹è·¯å¾„ï¼ŒæŠ¥é”™æç¤ºç”¨æˆ·é…ç½®
        if not self.model_path:
            console.print("[red]âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„æœªè®¾ç½®[/red]")
            console.print("[yellow]æç¤º: è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€æŒ‡å®šæ¨¡å‹ï¼š[/yellow]")
            console.print("[dim]  1. å‘½ä»¤è¡Œå‚æ•°: --model-path <model-name> æˆ– <local-path>[/dim]")
            console.print("[dim]  2. ç¯å¢ƒå˜é‡: åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® VLLM_MODEL=<model-name>[/dim]")
            console.print("[dim]  3. åŸºçº¿ç³»ç»Ÿå¿…é¡»ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä¸æ”¯æŒ API[/dim]")
            raise ValueError("æ¨¡å‹è·¯å¾„æœªè®¾ç½®ï¼Œæ— æ³•åˆå§‹åŒ–åŸºçº¿æ¨ç†å¼•æ“")
        
        # åˆå§‹åŒ–åŸºçº¿æ¨ç†å¼•æ“ï¼ˆBaselineInferenceï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ° transformers æ¨¡å¼ï¼‰
        if LOCAL_INFERENCE_AVAILABLE or True:
            try:
                console.print(f"[yellow]ğŸ“Š ä½¿ç”¨åŸºçº¿æ¨ç†å¼•æ“ï¼ˆBaselineInferenceï¼Œæœ¬åœ° transformers æ¨¡å¼ï¼‰[/yellow]")
                console.print(f"[dim]æ¨¡å‹: {self.model_path}[/dim]")
                
                # åŸºçº¿ç³»ç»Ÿå¼ºåˆ¶ä½¿ç”¨æœ¬åœ° transformers æ¨¡å‹ï¼Œä¸ä½¿ç”¨ API
                # BaselineInference ä¼šè‡ªåŠ¨æ£€æµ‹ï¼šå¦‚æœ base_url æ˜¯æœ¬åœ° vLLM æœåŠ¡ä¸”æ¨¡å‹è·¯å¾„æ˜¯æœ¬åœ°è·¯å¾„ï¼Œ
                # åˆ™å¼ºåˆ¶ä½¿ç”¨æœ¬åœ° transformers æ¨¡å¼ï¼ˆä¸é€šè¿‡ vLLM APIï¼‰
                self.inference_engine = BaselineInference(
                    model_path=self.model_path,
                    base_url=self.settings.vllm_base_url,  # ä¼ é€’ base_urlï¼Œä½† BaselineInference ä¼šåˆ¤æ–­ä½¿ç”¨æœ¬åœ°æ¨¡å¼
                    api_key=self.settings.vllm_api_key,
                    timeout=self.settings.vllm_timeout
                )
                self.use_llm = True
                
                # æ˜¾ç¤ºä½¿ç”¨çš„æ¨¡å¼
                if hasattr(self.inference_engine, 'use_api') and self.inference_engine.use_api:
                    console.print(f"[dim]æ¨¡å¼: APIï¼ˆ{self.inference_engine.base_url}ï¼‰[/dim]")
                else:
                    console.print(f"[dim]æ¨¡å¼: æœ¬åœ°æ¨¡å‹[/dim]")
            except Exception as e:
                console.print(f"[red]âŒ åŸºçº¿æ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}[/red]")
                import traceback
                traceback.print_exc()
                self.inference_engine = None
                self.use_llm = False
                console.print("\n[red]âŒ åŸºçº¿ç³»ç»Ÿå¿…é¡»ä½¿ç”¨ BaselineInferenceï¼Œç¨‹åºé€€å‡º[/red]")
                sys.exit(1)
        else:
            console.print("[red]âŒ æ— æ³•ä½¿ç”¨åŸºçº¿æ¨ç†å¼•æ“[/red]")
            console.print("[dim]æç¤º: è¯·å®‰è£…å¿…è¦çš„ä¾èµ–ï¼ˆtransformers æˆ– openaiï¼‰[/dim]")
            self.inference_engine = None
            self.use_llm = False
            console.print("\n[red]âŒ åŸºçº¿ç³»ç»Ÿå¿…é¡»ä½¿ç”¨ BaselineInferenceï¼Œç¨‹åºé€€å‡º[/red]")
            sys.exit(1)
        
        # æ€§èƒ½è®°å½•
        self.latencies = []
        self.ttfts = []  # é¦–å­—å»¶è¿Ÿ
        self.tokens_per_second = []  # æ¯ç§’ tokensï¼ˆååé‡ï¼‰
        self.tokens_generated = []  # æ¯æ¬¡ç”Ÿæˆçš„ tokens æ•°
        self.tokens_per_second = []  # æ¯ç§’ tokensï¼ˆååé‡ï¼‰
        self.tokens_generated = []  # æ¯æ¬¡ç”Ÿæˆçš„ tokens æ•°
        
        # é•¿æœŸè®°å¿†å¬å›æµ‹è¯•è®°å½•ï¼ˆåŸºçº¿ç³»ç»Ÿæ— æ³•å¬å›æ—©æœŸä¿¡æ¯ï¼‰
        self.recall_test_cases = {
            "å·¥å·": {"query": "æˆ‘çš„å·¥å·", "expected": "9527", "found": False, "retrieved_content": ""},
            "å’–å•¡ä¹ æƒ¯": {"query": "æˆ‘å–å’–å•¡çš„ä¹ æƒ¯", "expected": "å†°ç¾å¼ï¼Œä¸åŠ ç³–", "found": False, "retrieved_content": ""},
            "è®¢å•å·": {"query": "æŸ¥è¯¢çš„è®¢å•å·", "expected": "ORDER_2026_001", "found": False, "retrieved_content": ""},
        }
    
    def generate_script(self) -> List[str]:
        """ç”Ÿæˆ 100 è½®æ¨¡æ‹Ÿå‰§æœ¬ï¼ˆä¸å¢å¼ºç‰ˆç›¸åŒï¼‰"""
        script = []
        
        # Phase 1: è®¾å®šäººè®¾ (1-5è½®)
        script.extend([
            "ä½ å¥½ï¼Œæˆ‘æ˜¯æµ‹è¯•å‘˜é˜¿å¼ºã€‚æˆ‘çš„å·¥å·æ˜¯ 9527ã€‚",
            "æˆ‘å–œæ¬¢å–å†°ç¾å¼ï¼Œä¸åŠ ç³–ã€‚",
            "æˆ‘ä»¬ä»Šå¤©çš„ä»»åŠ¡æ˜¯æµ‹è¯• Agent çš„æé™æ€§èƒ½ã€‚",
            "ä½ åªéœ€è¦ç®€çŸ­å›å¤æ”¶åˆ°å³å¯ã€‚",
            "å‡†å¤‡å¥½äº†å—ï¼Ÿ"
        ])
        
        # Phase 2: ç–¯ç‹‚å¤è¯» (6-35è½®)
        for _ in range(10):
            script.append("å¸®æˆ‘æŸ¥ä¸€ä¸‹è®¢å• ORDER_2026_001 çš„çŠ¶æ€")
            script.append("è®¢å• ORDER_2026_001 å‘è´§äº†å—ï¼Ÿ")
            script.append("è¿˜æ²¡æŸ¥åˆ° ORDER_2026_001 å—ï¼Ÿ")
            
        # Phase 3: ä½ä»·å€¼çŒæ°´ (36-85è½®)
        fillers = ["å¥½çš„", "æ”¶åˆ°", "æ˜ç™½", "OK", "è°¢è°¢", "åœ¨å—", "å—¯å—¯", "ä¸é”™", "å“ˆå“ˆ", "ç¡®å®"]
        for i in range(50):
            script.append(fillers[i % len(fillers)])
            
        # Phase 4: è®°å¿†çªå‡»æ£€æŸ¥ (86-100è½®)
        script.extend([
            "æµ‹è¯•å¿«ç»“æŸäº†ï¼Œç¨å¾®æ€»ç»“ä¸€ä¸‹ã€‚",
            "å¯¹äº†ï¼Œè¿˜è®°å¾—æˆ‘æ˜¯è°å—ï¼ŸæŠ¥ä¸Šæˆ‘çš„å·¥å·ã€‚",
            "æˆ‘åˆšæ‰è¯´æˆ‘å–å’–å•¡æœ‰ä»€ä¹ˆä¹ æƒ¯ï¼Ÿ",
            "æˆ‘ä»¬åˆšæ‰æŸ¥è¯¢äº†å“ªä¸ªè®¢å•å·ï¼Ÿ",
            "è¿™ä¸€ç™¾å¥èŠä¸‹æ¥ï¼Œä½ ç´¯ä¸ç´¯ï¼Ÿ",
            "å†è§ï¼"
        ])
        
        return script
    
    def call_llm(self, user_input: str, context: List[str]) -> tuple[str, float]:
        """
        è°ƒç”¨åŸºçº¿æ¨ç†å¼•æ“ç”Ÿæˆå›å¤ï¼ˆä½¿ç”¨ BaselineInferenceï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ˆåŸºçº¿ç³»ç»Ÿåªæœ‰æ»‘åŠ¨çª—å£å†…çš„å¯¹è¯ï¼‰
            
        Returns:
            (ç”Ÿæˆçš„å›å¤, æ€»å»¶è¿Ÿæ—¶é—´(ms))
        """
        if not self.use_llm or not self.inference_engine:
            console.print("[red]âŒ æ¨ç†å¼•æ“ä¸å¯ç”¨ï¼Œç¨‹åºé€€å‡º[/red]")
            sys.exit(1)
        
        # æ„å»ºæç¤ºè¯ï¼ˆä¸å¢å¼ºç‰ˆç›¸åŒï¼‰
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç®€æ´ã€å‡†ç¡®åœ°å›å¤ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆåŸºçº¿ç³»ç»Ÿåªæœ‰æ»‘åŠ¨çª—å£å†…çš„å¯¹è¯ï¼‰
        context_text = ""
        if context:
            context_text = "\n".join([f"- {doc}" for doc in context[:5]])  # æœ€å¤šä½¿ç”¨5æ¡ä¸Šä¸‹æ–‡
            context_text = f"ç›¸å…³ä¸Šä¸‹æ–‡ï¼š\n{context_text}\n\n"
        
        user_prompt = f"{context_text}ç”¨æˆ·é—®é¢˜ï¼š{user_input}"
        
        try:
            # è°ƒç”¨åŸºçº¿æ¨ç†å¼•æ“
            response, metrics = self.inference_engine.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.7,
                max_tokens=512  # max_model_len=1024ï¼Œå¯ä»¥è®¾ç½®æ›´å¤§çš„å€¼
            )
            
            # æå–å»¶è¿Ÿä¿¡æ¯ï¼ˆä¸å¢å¼ºç‰ˆç›¸åŒçš„æŒ‡æ ‡ï¼‰
            total_time = metrics.get("total_time", 0) * 1000  # è½¬æ¢ä¸º ms
            ttft = metrics.get("ttft", 0) * 1000  # è½¬æ¢ä¸º ms
            tps = metrics.get("tokens_per_second", 0)  # æ¯ç§’ tokens
            tokens = metrics.get("tokens_generated", 0)  # ç”Ÿæˆçš„ tokens æ•°
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡ï¼ˆä¸å¢å¼ºç‰ˆç›¸åŒï¼‰
            if ttft > 0:
                self.ttfts.append(ttft)
            if tps > 0:
                self.tokens_per_second.append(tps)
            if tokens > 0:
                self.tokens_generated.append(tokens)
            
            return response, total_time
            
        except Exception as e:
            console.print(f"[red]âŒ åŸºçº¿æ¨ç†å¼•æ“è°ƒç”¨å¤±è´¥: {e}[/red]")
            import traceback
            traceback.print_exc()
            console.print("\n[red]âŒ æ¨ç†å¼•æ“è°ƒç”¨å¤±è´¥ï¼Œç¨‹åºé€€å‡º[/red]")
            sys.exit(1)
    
    def _test_recall(self, user_input: str, context_docs: List[str], response: str):
        """
        æµ‹è¯•å¬å›èƒ½åŠ›ï¼ˆåŸºçº¿ç³»ç»Ÿåªæœ‰æ»‘åŠ¨çª—å£å†…çš„å¯¹è¯ï¼Œæ— æ³•å¬å›æ—©æœŸä¿¡æ¯ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£ï¼ˆåŸºçº¿ç³»ç»Ÿåªæœ‰çª—å£å†…çš„å¯¹è¯ï¼‰
            response: Agent å›å¤
        """
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ç”¨äºæ£€æŸ¥
        all_text = " ".join(context_docs) + " " + response
        
        # æ£€æŸ¥å·¥å·å¬å›ï¼ˆç¬¬1è½®çš„ä¿¡æ¯ï¼Œçª—å£å¤§å°ä¸º10æ—¶å·²ä¸åœ¨çª—å£å†…ï¼‰
        if ("å·¥å·" in user_input or "æˆ‘æ˜¯è°" in user_input) and not self.recall_test_cases["å·¥å·"]["found"]:
            if "9527" in all_text:
                self.recall_test_cases["å·¥å·"]["found"] = True
                for doc in context_docs:
                    if "9527" in doc:
                        self.recall_test_cases["å·¥å·"]["retrieved_content"] = doc[:150]
                        break
                if not self.recall_test_cases["å·¥å·"]["retrieved_content"]:
                    self.recall_test_cases["å·¥å·"]["retrieved_content"] = response[:150]
        
        # æ£€æŸ¥å’–å•¡ä¹ æƒ¯å¬å›ï¼ˆç¬¬2è½®çš„ä¿¡æ¯ï¼‰
        if ("å’–å•¡" in user_input or ("å–" in user_input and "ä¹ æƒ¯" in user_input)) and not self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["found"]:
            if "å†°ç¾å¼" in all_text or "ä¸åŠ ç³–" in all_text:
                self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["found"] = True
                for doc in context_docs:
                    if "å†°ç¾å¼" in doc or "ä¸åŠ ç³–" in doc:
                        self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["retrieved_content"] = doc[:150]
                        break
                if not self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["retrieved_content"]:
                    self.recall_test_cases["å’–å•¡ä¹ æƒ¯"]["retrieved_content"] = response[:150]
        
        # æ£€æŸ¥è®¢å•å·å¬å›ï¼ˆç¬¬6-35è½®çš„ä¿¡æ¯ï¼Œå¯èƒ½åœ¨çª—å£å†…ï¼‰
        if ("è®¢å•" in user_input and "å“ªä¸ª" in user_input) and not self.recall_test_cases["è®¢å•å·"]["found"]:
            if "ORDER_2026_001" in all_text or "ORDER_2024_001" in all_text:
                self.recall_test_cases["è®¢å•å·"]["found"] = True
                for doc in context_docs:
                    if "ORDER_2026_001" in doc or "ORDER_2024_001" in doc:
                        self.recall_test_cases["è®¢å•å·"]["retrieved_content"] = doc[:150]
                        break
                if not self.recall_test_cases["è®¢å•å·"]["retrieved_content"]:
                    self.recall_test_cases["è®¢å•å·"]["retrieved_content"] = response[:150]

    def run(self):
        script = self.generate_script()
        console.print(f"[bold yellow]ğŸ“Š Baseline Auto-Player å¯åŠ¨ï¼ˆå¯¹ç…§ç»„ï¼‰ï¼ç›®æ ‡: {len(script)} è½®å¯¹è¯[/bold yellow]")
        console.print(f"Session ID: {self.session_id}")
        
        if not self.use_llm:
            console.print("[red]âŒ æ¨ç†å¼•æ“æœªåˆå§‹åŒ–ï¼Œç¨‹åºé€€å‡º[/red]")
            sys.exit(1)
        
        llm_status = "âœ… BaselineInference" if self.use_llm else "âŒ æœªåˆå§‹åŒ–"
        console.print(f"é…ç½®: æ»‘åŠ¨çª—å£å¤§å°={self.agent.window_size}, LLM={llm_status}, æ¨¡å‹={self.model_path}")
        console.print(f"[dim]âš ï¸  åŸºçº¿ç³»ç»Ÿï¼šæ— é•¿æœŸè®°å¿†ã€æ— è¯­ä¹‰æ£€ç´¢ã€æ— å­˜å‚¨ä¼˜åŒ–ã€æ—  vLLM ä¼˜åŒ–[/dim]")
        print("-" * 60)

        # åŸºçº¿ç³»ç»Ÿæ²¡æœ‰å‘é‡æ•°æ®åº“ï¼Œè®°å½•å¯¹è¯å†å²å¤§å°
        start_history_size = len(self.agent.conversation_history)

        for i, user_input in enumerate(track(script, description="æ­£åœ¨äº¤äº’ï¼ˆåŸºçº¿ï¼‰...")):
            start_time = time.time()
            
            # è·å–ä¸Šä¸‹æ–‡ï¼ˆåªæœ‰æ»‘åŠ¨çª—å£å†…çš„å¯¹è¯ï¼‰
            context_docs = self.agent.get_context(user_input)
            
            # è°ƒç”¨ LLM ç”Ÿæˆå›å¤
            response, llm_latency = self.call_llm(user_input, context_docs)
            
            # ä¿å­˜å¯¹è¯åˆ°æ»‘åŠ¨çª—å£
            self.agent.add_conversation(user_input, response)
            
            end_time = time.time()
            total_latency = (end_time - start_time) * 1000  # ms
            
            self.latencies.append(total_latency)
            
            # æµ‹è¯•å¬å›èƒ½åŠ›ï¼ˆPhase 4ï¼‰
            if i >= 85:
                self._test_recall(user_input, context_docs, response)
            
            # ç®€å•æ‰“å°äº¤äº’
            if i < 5 or i > 85 or i % 20 == 0:
                console.print(f"[blue]User({i+1}):[/blue] {user_input}")
                ttft_info = f", TTFT: {self.ttfts[-1]:.1f}ms" if self.ttfts else ""
                console.print(f"[yellow]Agent:[/yellow] {response[:80]}... [dim](æ€»å»¶è¿Ÿ: {total_latency:.1f}ms{ttft_info}, çª—å£å†…å¯¹è¯{len(context_docs)}æ¡)[/dim]")

        # è·å–æœ€ç»ˆçŠ¶æ€
        end_history_size = len(self.agent.conversation_history)
        stats = self.agent.get_stats()
        total_turns = len(script)
        stored_turns = end_history_size
        
        # è®¡ç®—å­˜å‚¨ç»Ÿè®¡
        # åŸºçº¿ç³»ç»Ÿï¼šæ‰€æœ‰å¯¹è¯éƒ½ä¿å­˜åœ¨çª—å£å†…ï¼ˆè¶…å‡ºçª—å£çš„ä¸¢å¤±ï¼‰
        # å™ªéŸ³è¿‡æ»¤ç‡ï¼šåŸºçº¿ç³»ç»Ÿæ²¡æœ‰è¿‡æ»¤ï¼Œæ‰€ä»¥è¿‡æ»¤ç‡ä¸º0
        noise_filter_rate = 0.0
        
        # ä¿¡æ¯ä¿ç•™ç‡ï¼šåªä¿ç•™çª—å£å†…çš„å¯¹è¯
        retention_rate = (stored_turns / total_turns * 100) if total_turns > 0 else 0
        
        self.generate_report(len(script), start_history_size, end_history_size, stats, 
                           stored_turns, noise_filter_rate, retention_rate)

    def generate_report(self, total_turns, start_history_size, end_history_size, stats,
                       stored_turns, noise_filter_rate, retention_rate):
        """
        ç”ŸæˆæŠ¥å‘Š
        
        Args:
            total_turns: æ€»å¯¹è¯è½®æ•°
            start_history_size: åˆå§‹å†å²å¤§å°
            end_history_size: æœ€ç»ˆå†å²å¤§å°
            stats: ç»Ÿè®¡ä¿¡æ¯
            stored_turns: å­˜å‚¨çš„å¯¹è¯æ•°
            noise_filter_rate: å™ªéŸ³è¿‡æ»¤ç‡
            retention_rate: ä¿¡æ¯ä¿ç•™ç‡
        """
        console.print("\n\n")
        console.rule("[bold yellow]ğŸ“Š åŸºçº¿ç³»ç»ŸéªŒè¯æŠ¥å‘Šï¼ˆå¯¹ç…§ç»„ï¼‰[/bold yellow]")
        
        # 1. æ€§èƒ½æŒ‡æ ‡ï¼ˆä¸å¢å¼ºç‰ˆç›¸åŒï¼‰
        avg_latency = statistics.mean(self.latencies) if self.latencies else 0
        p95_latency = statistics.quantiles(self.latencies, n=20)[18] if len(self.latencies) >= 20 else 0
        
        perf_table = Table(title="âš¡ æ¨ç†æ€§èƒ½ (Inference Speed)")
        perf_table.add_column("æŒ‡æ ‡", style="cyan")
        perf_table.add_column("ç»“æœ", style="green")
        perf_table.add_column("è¯„ä»·")
        
        # è®¡ç®—é¦–å­—å»¶è¿Ÿç»Ÿè®¡
        avg_ttft = statistics.mean(self.ttfts) if self.ttfts else 0
        p95_ttft = statistics.quantiles(self.ttfts, n=20)[18] if len(self.ttfts) >= 20 else 0
        
        perf_table.add_row("å¹³å‡å»¶è¿Ÿ", f"{avg_latency:.1f} ms", "ğŸš€ ç§’å›" if avg_latency < 500 else "âš ï¸ æ­£å¸¸")
        perf_table.add_row("P95 å»¶è¿Ÿ", f"{p95_latency:.1f} ms", "åœ¨é«˜è´Ÿè½½ä¸‹è¡¨ç°ç¨³å®š")
        if avg_ttft > 0:
            perf_table.add_row("å¹³å‡é¦–å­—å»¶è¿Ÿ (TTFT)", f"{avg_ttft:.1f} ms", "ğŸš€ æå¿«" if avg_ttft < 200 else "âœ… è‰¯å¥½")
            perf_table.add_row("P95 é¦–å­—å»¶è¿Ÿ", f"{p95_ttft:.1f} ms", "é«˜è´Ÿè½½ä¸‹é¦–å­—å“åº”ç¨³å®š")
        
        # ååé‡æŒ‡æ ‡
        avg_tps = statistics.mean(self.tokens_per_second) if self.tokens_per_second else 0
        p95_tps = statistics.quantiles(self.tokens_per_second, n=20)[18] if len(self.tokens_per_second) >= 20 else 0
        
        if avg_tps > 0:
            perf_table.add_row("å¹³å‡ååé‡", f"{avg_tps:.1f} tokens/s", "ğŸš€ ä¼˜ç§€" if avg_tps > 50 else "âœ… è‰¯å¥½")
            perf_table.add_row("P95 ååé‡", f"{p95_tps:.1f} tokens/s", "é«˜è´Ÿè½½ä¸‹ååé‡ç¨³å®š")
        
        perf_table.add_row("æ€»è€—æ—¶", f"{sum(self.latencies)/1000:.1f} s", f"å¤„ç† {total_turns} è½®å¯¹è¯")
        
        console.print(perf_table)
        console.print("\n")

        # 2. å­˜å‚¨æŒ‡æ ‡ï¼ˆåŸºçº¿ç³»ç»Ÿï¼šæ‰€æœ‰å¯¹è¯éƒ½ä¿å­˜åœ¨çª—å£å†…ï¼Œè¶…å‡ºçª—å£çš„ä¸¢å¤±ï¼‰
        lost_turns = total_turns - stored_turns
        
        store_table = Table(title="ğŸ’¾ å­˜å‚¨æƒ…å†µ (Storage)")
        store_table.add_column("æŒ‡æ ‡", style="cyan")
        store_table.add_column("æ•°æ®", style="magenta")
        store_table.add_column("è¯´æ˜")
        
        store_table.add_row("è¾“å…¥å¯¹è¯æ€»æ•°", str(total_turns), "æ¨¡æ‹Ÿçš„ç”¨æˆ·è¾“å…¥")
        store_table.add_row("çª—å£ä¸­å­˜å‚¨æ•°", str(stored_turns), f"çª—å£å†…ä¿ç•™çš„å¯¹è¯æ•°ï¼ˆçª—å£å¤§å°: {stats['window_size']}ï¼‰")
        store_table.add_row("ä¸¢å¤±çš„å¯¹è¯æ•°", str(lost_turns), f"è¶…å‡ºçª—å£çš„å¯¹è¯å·²ä¸¢å¤±")
        store_table.add_row("å™ªéŸ³è¿‡æ»¤ç‡", f"{noise_filter_rate:.1f}%", "åŸºçº¿ç³»ç»Ÿæ— è¿‡æ»¤åŠŸèƒ½ï¼Œæ‰€æœ‰å¯¹è¯éƒ½å­˜å‚¨ï¼ˆåœ¨çª—å£å†…ï¼‰")
        store_table.add_row("ä¿¡æ¯ä¿ç•™ç‡", f"{retention_rate:.1f}%", "ä»…ä¿ç•™æœ€è¿‘çª—å£å†…çš„å¯¹è¯")
        
        console.print(store_table)
        console.print("\n")
        
        # 3. é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›ï¼ˆåŸºçº¿ç³»ç»Ÿæ— æ³•å¬å›æ—©æœŸä¿¡æ¯ï¼‰
        recall_table = Table(title="ğŸ§  é•¿æœŸè®°å¿†å¬å›èƒ½åŠ› (Long-Term Memory Recall)")
        recall_table.add_column("æµ‹è¯•é¡¹", style="cyan", width=12)
        recall_table.add_column("æœŸæœ›ä¿¡æ¯", style="yellow", width=20)
        recall_table.add_column("å¬å›çŠ¶æ€", style="green", width=12)
        recall_table.add_column("æ£€ç´¢å†…å®¹é¢„è§ˆ", style="dim", width=40)
        
        recall_success_count = 0
        for test_name, test_case in self.recall_test_cases.items():
            status = "âœ… æˆåŠŸ" if test_case["found"] else "âŒ å¤±è´¥"
            status_style = "green" if test_case["found"] else "red"
            if test_case["found"]:
                recall_success_count += 1
            
            retrieved_preview = test_case["retrieved_content"][:50] + "..." if test_case["retrieved_content"] else "[dim]æœªæ£€ç´¢åˆ°ï¼ˆè¶…å‡ºçª—å£ï¼‰[/dim]"
            
            recall_table.add_row(
                test_name,
                test_case["expected"],
                f"[{status_style}]{status}[/{status_style}]",
                retrieved_preview
            )
        
        recall_rate = (recall_success_count / len(self.recall_test_cases)) * 100 if self.recall_test_cases else 0
        
        console.print(recall_table)
        
        console.print(f"\n[bold cyan]ğŸ“Š å¬å›èƒ½åŠ›åˆ†æï¼š[/bold cyan]")
        console.print(f"  â€¢ æµ‹è¯•åœºæ™¯ï¼šåœ¨ {total_turns} è½®å¯¹è¯åï¼Œæµ‹è¯•ç³»ç»Ÿæ˜¯å¦èƒ½å¬å›æ—©æœŸï¼ˆç¬¬1-5è½®ï¼‰çš„å…³é”®ä¿¡æ¯")
        console.print(f"  â€¢ å¬å›æˆåŠŸç‡ï¼š{recall_rate:.1f}% ({recall_success_count}/{len(self.recall_test_cases)})")
        console.print(f"  â€¢ åŸºçº¿ç³»ç»Ÿé™åˆ¶ï¼šæ»‘åŠ¨çª—å£å¤§å° {stats['window_size']}ï¼Œè¶…å‡ºçª—å£çš„å¯¹è¯å·²ä¸¢å¤±")
        console.print(f"  â€¢ é¢„æœŸç»“æœï¼šæ—©æœŸä¿¡æ¯ï¼ˆå·¥å·ã€å’–å•¡ä¹ æƒ¯ï¼‰æ— æ³•å¬å›ï¼Œå› ä¸ºå·²è¶…å‡ºçª—å£")
        console.print()
        
        # 4. ç»“è®º
        console.rule("[bold]ğŸ“ æœ€ç»ˆç»“è®º[/bold]")
        if avg_latency < 2000:
            console.print("âœ… [bold green]åŸºçº¿æ¨ç†æ­£å¸¸ï¼š[/bold green] å“åº”é€Ÿåº¦è‰¯å¥½ã€‚")
        else:
            console.print("âš ï¸ [bold yellow]åŸºçº¿å»¶è¿Ÿè¾ƒé«˜ï¼š[/bold yellow] è¯·æ£€æŸ¥ GPU é…ç½®æˆ–æ¨¡å‹è·¯å¾„ã€‚")
        
        if avg_ttft > 0 and avg_ttft < 500:
            console.print(f"âœ… [bold green]é¦–å­—å»¶è¿Ÿè‰¯å¥½ï¼š[/bold green] å¹³å‡ TTFT {avg_ttft:.1f}msã€‚")
        elif avg_ttft > 0:
            console.print(f"âš ï¸ [bold yellow]é¦–å­—å»¶è¿Ÿï¼š[/bold yellow] å¹³å‡ TTFT {avg_ttft:.1f}msï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–é…ç½®ã€‚")
        
        console.print("âš ï¸ [bold yellow]åŸºçº¿ç³»ç»Ÿé™åˆ¶ï¼š[/bold yellow] æ— é•¿æœŸè®°å¿†ã€æ— è¯­ä¹‰æ£€ç´¢ã€æ— å­˜å‚¨ä¼˜åŒ–ï¼Œæ‰€æœ‰å¯¹è¯éƒ½å­˜å‚¨åœ¨æ»‘åŠ¨çª—å£å†…ã€‚")
        
        # é•¿æœŸè®°å¿†å¬å›èƒ½åŠ›è¯„ä»·
        if recall_rate >= 100:
            console.print(f"âœ… [bold green]å¬å›èƒ½åŠ›è‰¯å¥½ï¼š[/bold green] å¬å›æˆåŠŸç‡ {recall_rate:.1f}%ï¼Œæ‰€æœ‰å…³é”®ä¿¡æ¯éƒ½èƒ½æ­£ç¡®å¬å›ã€‚")
        elif recall_rate > 0:
            console.print(f"âš ï¸ [bold yellow]å¬å›èƒ½åŠ›æœ‰é™ï¼š[/bold yellow] å¬å›æˆåŠŸç‡ {recall_rate:.1f}%ï¼Œåªèƒ½å¬å›çª—å£å†…çš„ä¿¡æ¯ã€‚")
        else:
            console.print(f"âŒ [bold red]å¬å›èƒ½åŠ›ä¸è¶³ï¼š[/bold red] å¬å›æˆåŠŸç‡ 0%ï¼Œæ—©æœŸä¿¡æ¯å·²è¶…å‡ºçª—å£ï¼Œæ— æ³•å¬å›ã€‚")

    def close(self) -> None:
        """æ¸…ç†èµ„æº"""
        pass  # åŸºçº¿ç³»ç»Ÿæ— éœ€æ¸…ç†


if __name__ == "__main__":
    import argparse
    import atexit
    import signal
    
    def cleanup_resources():
        """æ¸…ç†èµ„æº"""
        try:
            if player is not None and hasattr(player, 'close'):
                player.close()
        except Exception:
            pass
    
    parser = argparse.ArgumentParser(description="Baseline Auto-Player: åŸºçº¿å¯¹ç…§ç»„è„šæœ¬")
    parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ VLLM_MODEL_PATH è®¾ç½®ï¼Œé»˜è®¤: Qwen/Qwen2.5-7B-Instructï¼‰"
    )
    parser.add_argument(
        "--window-size",
        type=int,
        default=10,
        help="æ»‘åŠ¨çª—å£å¤§å°ï¼ˆé»˜è®¤: 10ï¼‰"
    )
    
    args = parser.parse_args()
    
    player = None
    try:
        player = BaselineAutoPlayer(
            model_path=args.model_path,
            window_size=args.window_size
        )
        
        atexit.register(cleanup_resources)
        
        def signal_handler(sig, frame):
            cleanup_resources()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        player.run()
    except KeyboardInterrupt:
        console.print("\n[yellow]âš ï¸  ç”¨æˆ·ä¸­æ–­[/yellow]")
        cleanup_resources()
        sys.exit(0)
    except Exception as e:
        console.print(f"[red]âŒ è¿è¡Œå‡ºé”™: {e}[/red]")
        cleanup_resources()
        sys.exit(1)
    finally:
        cleanup_resources()
