#!/usr/bin/env python3
"""
vLLM æœåŠ¡å¯åŠ¨è„šæœ¬ï¼ˆå¯é€‰ï¼‰

è¿™ä¸ªè„šæœ¬å¯ä»¥å¸®åŠ©ç”¨æˆ·è‡ªåŠ¨å¯åŠ¨ vLLM æœåŠ¡ã€‚
æ³¨æ„ï¼šæ¨èåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è®©ç”¨æˆ·æ‰‹åŠ¨å¯åŠ¨ vLLM æœåŠ¡ï¼ˆå¸¸é©»ï¼‰ï¼Œè€Œä¸æ˜¯æ¯æ¬¡æ’ä»¶å¯åŠ¨éƒ½é‡å¯ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/launch_vllm_server.py --model Qwen/Qwen2.5-7B-Instruct --port 8000

ç‰¹æ€§ï¼š
- è‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡é¿å… duplicate template name é”™è¯¯
- é»˜è®¤ç¦ç”¨ CUDA graphï¼ˆä½¿ç”¨ --enforce-eagerï¼‰ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§
- å‰ç¼€ç¼“å­˜ï¼ˆPrefix Cachingï¼‰ï¼šé»˜è®¤å¯ç”¨ï¼ˆå¯é€‰ï¼Œæå‡é‡å¤æç¤ºçš„æ€§èƒ½ï¼‰
- å¥åº·æ£€æŸ¥å’Œé”™è¯¯æç¤º

æ€§èƒ½ä¼˜åŒ–ï¼š
- CUDA graphï¼šé»˜è®¤ç¦ç”¨ï¼ˆä½¿ç”¨ --enforce-eagerï¼‰ï¼Œé¿å… duplicate template name é”™è¯¯
- å‰ç¼€ç¼“å­˜ï¼ˆPrefix Cachingï¼‰ï¼šé»˜è®¤å¯ç”¨ï¼ˆå¯é€‰ï¼Œæå‡é‡å¤æç¤ºçš„æ€§èƒ½ï¼‰

å¦‚éœ€å¯ç”¨ CUDA graph ä¼˜åŒ–ï¼Œå¯ä½¿ç”¨ --no-enforce-eager å‚æ•°å¯åŠ¨æœåŠ¡ï¼ˆå¯èƒ½è§¦å‘ duplicate template name é”™è¯¯ï¼‰ã€‚
"""

import subprocess
import time
import sys
import signal
import os
import threading
from pathlib import Path

try:
    import requests
except ImportError:
    print("âŒ requests åº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install requests")
    sys.exit(1)

try:
    from src.config import get_settings
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
    class DefaultSettings:
        vllm_model_path = None
        vllm_model = None
        vllm_gpu_memory_utilization = 0.4
        vllm_max_model_len = 1024  # é»˜è®¤ 1024
        hf_endpoint = None
    
    def get_settings():
        return DefaultSettings()


def check_vllm_installed() -> bool:
    """æ£€æŸ¥ vLLM æ˜¯å¦å·²å®‰è£…"""
    # æ–¹æ³•1: å°è¯•ç›´æ¥å¯¼å…¥ï¼ˆæœ€å¿«ï¼‰
    try:
        import vllm
        return True
    except ImportError:
        pass
    
    # æ–¹æ³•2: å°è¯•è¿è¡Œ vllm å‘½ä»¤ï¼ˆå¯èƒ½å› ä¸º duplicate template name å¤±è´¥ï¼Œä½†ä¸ä»£è¡¨æœªå®‰è£…ï¼‰
    try:
        result = subprocess.run(
            ["python", "-m", "vllm", "--help"],
            capture_output=True,
            timeout=5
        )
        # å³ä½¿è¿”å›é0ï¼Œä¹Ÿå¯èƒ½æ˜¯å…¶ä»–é”™è¯¯ï¼ˆå¦‚ duplicate template nameï¼‰ï¼Œä¸æ˜¯æœªå®‰è£…
        # æ‰€ä»¥æˆ‘ä»¬åªæ£€æŸ¥æ˜¯å¦èƒ½æ‰¾åˆ°å‘½ä»¤
        return True  # å¦‚æœèƒ½è¿è¡Œå‘½ä»¤ï¼ˆå³ä½¿æŠ¥é”™ï¼‰ï¼Œè¯´æ˜å·²å®‰è£…
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False
    except Exception:
        # å…¶ä»–å¼‚å¸¸ï¼ˆå¦‚ duplicate template nameï¼‰ï¼Œè¯´æ˜ vLLM å·²å®‰è£…ä½†æœ‰é—®é¢˜
        return True


def find_local_model(model_name: str) -> str:
    """
    æŸ¥æ‰¾æœ¬åœ°æ¨¡å‹è·¯å¾„
    
    ä¼˜å…ˆçº§ï¼š
    1. å¦‚æœ model_name æ˜¯ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
    2. æ£€æŸ¥ HuggingFace ç¼“å­˜ç›®å½•
    3. æ£€æŸ¥å¸¸è§çš„æ¨¡å‹å­˜å‚¨ä½ç½®
    4. å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸå§‹ model_nameï¼ˆè®© vLLM å¤„ç†ï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
    
    Returns:
        æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼‰æˆ–åŸå§‹ model_name
    """
    from pathlib import Path
    
    # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
    if os.path.isabs(model_name) or model_name.startswith('./') or model_name.startswith('../'):
        if os.path.exists(model_name):
            return model_name
        return model_name  # å³ä½¿ä¸å­˜åœ¨ä¹Ÿè¿”å›ï¼Œè®© vLLM æŠ¥é”™
    
    # æ£€æŸ¥ HuggingFace ç¼“å­˜
    hf_cache_dirs = [
        os.path.expanduser("~/.cache/huggingface/hub"),
        os.environ.get("HF_HOME", ""),
        os.environ.get("TRANSFORMERS_CACHE", ""),
    ]
    
    for cache_dir in hf_cache_dirs:
        if not cache_dir:
            continue
        
        # HuggingFace ç¼“å­˜è·¯å¾„æ ¼å¼ï¼šmodels--org--model_name
        # ä¾‹å¦‚ï¼šQwen/Qwen2.5-7B-Instruct -> models--Qwen--Qwen2.5-7B-Instruct
        cache_name = model_name.replace("/", "--")
        model_cache_path = os.path.join(cache_dir, f"models--{cache_name}")
        
        if os.path.exists(model_cache_path):
            # æŸ¥æ‰¾ snapshots ç›®å½•ä¸‹çš„æœ€æ–°ç‰ˆæœ¬
            snapshots_dir = os.path.join(model_cache_path, "snapshots")
            if os.path.exists(snapshots_dir):
                snapshots = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                if snapshots:
                    # ä½¿ç”¨æœ€æ–°çš„ snapshot
                    latest_snapshot = sorted(snapshots)[-1]
                    model_path = os.path.join(snapshots_dir, latest_snapshot)
                    if os.path.exists(os.path.join(model_path, "config.json")):
                        return model_path
    
    # æ£€æŸ¥å¸¸è§çš„æ¨¡å‹å­˜å‚¨ä½ç½®
    common_paths = [
        os.path.expanduser(f"~/models/{model_name}"),
        os.path.expanduser(f"~/autodl-tmp/models/{model_name}"),
        f"/root/models/{model_name}",
        f"/root/autodl-tmp/models/{model_name}",
        f"./models/{model_name}",
    ]
    
    for path in common_paths:
        if os.path.exists(path) and os.path.exists(os.path.join(path, "config.json")):
            return path
    
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸå§‹ model_nameï¼ˆè®© vLLM å¤„ç†ï¼Œå¯èƒ½ä¼šä¸‹è½½ï¼‰
    return model_name


def start_vllm_server(
    model: str = None,
    port: int = 8000,
    gpu_memory_utilization: float = None,
    max_model_len: int = None,
    enforce_eager: bool = True,  # é»˜è®¤ç¦ç”¨ CUDA graphï¼ˆé¿å… duplicate template name é”™è¯¯ï¼‰
    enable_prefix_caching: bool = True,  # é»˜è®¤å¯ç”¨å‰ç¼€ç¼“å­˜ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
    **kwargs
) -> subprocess.Popen:
    """
    å¯åŠ¨ vLLM æœåŠ¡
    
    Args:
        model: æ¨¡å‹è·¯å¾„æˆ– HuggingFace ID
        port: æœåŠ¡ç«¯å£
        gpu_memory_utilization: GPU å†…å­˜ä½¿ç”¨ç‡
        max_model_len: æœ€å¤§æ¨¡å‹é•¿åº¦
        enforce_eager: æ˜¯å¦ç¦ç”¨ CUDA graphï¼ˆé»˜è®¤ Trueï¼Œé¿å… duplicate template name é”™è¯¯ï¼‰
                      å¦‚æœç¯å¢ƒç¨³å®šï¼Œå¯è®¾ä¸º False ä»¥å¯ç”¨ CUDA graph æå‡æ€§èƒ½
        enable_prefix_caching: æ˜¯å¦å¯ç”¨å‰ç¼€ç¼“å­˜ï¼ˆé»˜è®¤ Trueï¼Œå¯ç”¨ä»¥æå‡æ€§èƒ½ï¼‰
        **kwargs: å…¶ä»– vLLM å¯åŠ¨å‚æ•°
    
    Returns:
        subprocess.Popen: æœåŠ¡è¿›ç¨‹å¯¹è±¡
    """
    settings = get_settings()
    
    # ä½¿ç”¨é…ç½®æˆ–å‚æ•°
    model = model or settings.vllm_model_path or settings.vllm_model
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œæç¤ºç”¨æˆ·
    if not model:
        print("âŒ æœªæŒ‡å®šæ¨¡å‹")
        print("   è¯·ä½¿ç”¨ --model å‚æ•°æŒ‡å®šæ¨¡å‹è·¯å¾„æˆ– HuggingFace ID")
        print("   ä¾‹å¦‚ï¼š")
        print("     --model /path/to/local/model  # æœ¬åœ°æ¨¡å‹è·¯å¾„")
        print("     --model Qwen/Qwen2.5-7B-Instruct  # HuggingFace ID")
        sys.exit(1)
    
    # å°è¯•æŸ¥æ‰¾æœ¬åœ°æ¨¡å‹
    original_model = model
    local_model = find_local_model(model)
    
    if local_model != original_model:
        print(f"   âœ“ æ‰¾åˆ°æœ¬åœ°æ¨¡å‹: {local_model}")
        model = local_model
    elif os.path.exists(model) or os.path.isabs(model) or model.startswith('./'):
        print(f"   âœ“ ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹è·¯å¾„: {model}")
    else:
        print(f"   âš ï¸  æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå°†ä½¿ç”¨ HuggingFace ID: {original_model}")
        print(f"      å¦‚æœç½‘ç»œä¸å¯è¾¾ï¼Œè¯·ï¼š")
        print(f"      1. ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°åæŒ‡å®šè·¯å¾„")
        print(f"      2. æˆ–é…ç½® HuggingFace é•œåƒæºï¼ˆHF_ENDPOINTï¼‰")
        model = original_model
    gpu_memory_utilization = (
        gpu_memory_utilization 
        if gpu_memory_utilization is not None 
        else getattr(settings, 'vllm_gpu_memory_utilization', 0.7)  # é»˜è®¤ 0.7ï¼ˆ70%ï¼‰
    )
    max_model_len = (
        max_model_len 
        if max_model_len is not None 
        else getattr(settings, 'vllm_max_model_len', 1024)  # é»˜è®¤ 1024
    )
    
    # æ£€æŸ¥ vLLM æ˜¯å¦å·²å®‰è£…
    if not check_vllm_installed():
        print("âŒ vLLM æœªå®‰è£…æˆ–æ— æ³•å¯¼å…¥")
        print("   è¯·è¿è¡Œ: pip install vllm>=0.6.0")
        print("   æ³¨æ„ï¼švLLM éœ€è¦ CUDA >= 11.8 ç¯å¢ƒ")
        print()
        print("   å¦‚æœå·²å®‰è£…ä½†ä»æ˜¾ç¤ºæ­¤é”™è¯¯ï¼Œå¯èƒ½æ˜¯å¯¼å…¥æ—¶é‡åˆ°é—®é¢˜")
        print("   è¯·å°è¯•ï¼š")
        print("   1. æ£€æŸ¥ Python ç¯å¢ƒæ˜¯å¦æ­£ç¡®")
        print("   2. æ£€æŸ¥ vLLM æ˜¯å¦æ­£ç¡®å®‰è£…: pip show vllm")
        print("   3. å°è¯•æ‰‹åŠ¨å¯¼å…¥: python -c 'import vllm'")
        sys.exit(1)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å… duplicate template name é”™è¯¯
    # è¿™äº›ç¯å¢ƒå˜é‡å¿…é¡»åœ¨å¯åŠ¨ vLLM ä¹‹å‰è®¾ç½®
    env = os.environ.copy()
    env["TORCH_COMPILE_DISABLE"] = "1"
    env["TORCHDYNAMO_DISABLE"] = "1"
    # ä¸º Triton ç¼“å­˜è®¾ç½®å”¯ä¸€ç›®å½•ï¼Œé¿å…å†²çª
    env["TRITON_CACHE_DIR"] = f"/tmp/triton_cache_vllm_{os.getpid()}"
    
    # é…ç½® HuggingFace é•œåƒæºï¼ˆå¦‚æœè®¾ç½®äº†ï¼‰
    settings = get_settings()
    if settings.hf_endpoint:
        env["HF_ENDPOINT"] = settings.hf_endpoint
        print(f"   ğŸŒ ä½¿ç”¨ HuggingFace é•œåƒæº: {settings.hf_endpoint}")
    else:
        print(f"   âš ï¸  æœªé…ç½® HuggingFace é•œåƒæºï¼Œå¦‚æœç½‘ç»œä¸å¯è¾¾ï¼Œè¯·è®¾ç½® HF_ENDPOINT")
        print(f"      ä¾‹å¦‚: export HF_ENDPOINT=https://hf-mirror.com")
    
    # æ„å»ºå¯åŠ¨å‘½ä»¤
    # ä½¿ç”¨ vllm serve å‘½ä»¤ï¼ˆæ¨èï¼‰
    cmd = [
        "python", "-m", "vllm.entrypoints.openai.api_server",
        "--model", model,
        "--port", str(port),
        "--gpu-memory-utilization", str(gpu_memory_utilization),
        "--max-model-len", str(max_model_len),
    ]
    
    # æ·»åŠ  enforce-eager å‚æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼Œä¼šç¦ç”¨ CUDA graphï¼‰
    if enforce_eager:
        cmd.append("--enforce-eager")
        print("   âš ï¸  ä½¿ç”¨ --enforce-eager æ¨¡å¼ï¼ˆç¦ç”¨ CUDA graphï¼Œå¯èƒ½å½±å“æ€§èƒ½ï¼‰")
    else:
        print("   âœ… å¯ç”¨ CUDA graph ä¼˜åŒ–ï¼ˆæå‡æ¨ç†æ€§èƒ½ï¼‰")
    
    # æ·»åŠ å‰ç¼€ç¼“å­˜å‚æ•°ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
    if enable_prefix_caching:
        cmd.append("--enable-prefix-caching")
        print("   âœ… å¯ç”¨å‰ç¼€ç¼“å­˜ï¼ˆPrefix Cachingï¼‰ä¼˜åŒ–ï¼ˆæå‡é‡å¤æç¤ºçš„æ€§èƒ½ï¼‰")
    
    # æ·»åŠ å…¶ä»–å‚æ•°
    if "trust_remote_code" in kwargs and kwargs["trust_remote_code"]:
        cmd.append("--trust-remote-code")
    
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨ vLLM æœåŠ¡...")
    print(f"   æ¨¡å‹: {model}")
    print(f"   ç«¯å£: {port}")
    print(f"   GPU å†…å­˜ä½¿ç”¨ç‡: {gpu_memory_utilization}")
    print(f"   æœ€å¤§æ¨¡å‹é•¿åº¦: {max_model_len}")
    print(f"   å‘½ä»¤: {' '.join(cmd)}")
    print()
    
    # å¯åŠ¨æœåŠ¡ï¼ˆåå°è¿è¡Œï¼Œå®æ—¶æ˜¾ç¤ºæ—¥å¿—ï¼‰
    print("â³ æ­£åœ¨å¯åŠ¨ vLLM æœåŠ¡ï¼ˆé¦–æ¬¡å¯åŠ¨å¯èƒ½éœ€è¦ 1-3 åˆ†é’Ÿï¼‰...")
    print("   æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° GPUï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print()
    
    try:
        # ä½¿ç”¨å®æ—¶è¾“å‡ºæ¨¡å¼ï¼Œè®©ç”¨æˆ·çœ‹åˆ°å¯åŠ¨è¿›åº¦
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            env=env,  # ä¼ é€’ç¯å¢ƒå˜é‡
        )
        
        # ç­‰å¾…æœåŠ¡å°±ç»ª
        base_url = f"http://localhost:{port}"
        health_url = f"{base_url}/health"
        models_url = f"{base_url}/v1/models"
        
        max_wait = 600  # æœ€å¤šç­‰å¾… 10 åˆ†é’Ÿï¼ˆé¦–æ¬¡å¯åŠ¨å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
        wait_interval = 3  # æ¯ 3 ç§’æ£€æŸ¥ä¸€æ¬¡
        last_log_time = time.time()
        log_buffer = []
        
        print("ğŸ“‹ å¯åŠ¨æ—¥å¿—ï¼ˆå®æ—¶æ˜¾ç¤ºï¼‰ï¼š")
        print("-" * 60)
        
        for attempt in range(max_wait // wait_interval):
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            if process.poll() is not None:
                # è¿›ç¨‹å·²é€€å‡ºï¼Œè¯»å–å‰©ä½™è¾“å‡º
                remaining_output = process.stdout.read()
                if remaining_output:
                    print(remaining_output, end='')
                
                print("-" * 60)
                print(f"âŒ vLLM æœåŠ¡å¯åŠ¨å¤±è´¥")
                print(f"   é€€å‡ºç : {process.returncode}")
                print()
                print("ğŸ’¡ å¯èƒ½çš„åŸå› ï¼š")
                print("   1. GPU å†…å­˜ä¸è¶³ï¼ˆå°è¯•é™ä½ --gpu-memory-utilizationï¼‰")
                print("   2. æ¨¡å‹è·¯å¾„é”™è¯¯æˆ–æ¨¡å‹æ–‡ä»¶æŸå")
                print("   3. CUDA ç‰ˆæœ¬ä¸å…¼å®¹")
                print("   4. ç«¯å£è¢«å ç”¨ï¼ˆå°è¯•ä½¿ç”¨å…¶ä»–ç«¯å£ï¼š--port 8001ï¼‰")
                sys.exit(1)
            
            # å®æ—¶è¯»å–å¹¶æ˜¾ç¤ºæ—¥å¿—ï¼ˆéé˜»å¡ï¼‰
            import select
            import fcntl
            
            # è®¾ç½®éé˜»å¡æ¨¡å¼
            try:
                flags = fcntl.fcntl(process.stdout.fileno(), fcntl.F_GETFL)
                fcntl.fcntl(process.stdout.fileno(), fcntl.F_SETFL, flags | os.O_NONBLOCK)
                
                # è¯»å–å¯ç”¨è¾“å‡º
                while True:
                    line = process.stdout.readline()
                    if not line:
                        break
                    line = line.rstrip()
                    if line:
                        print(f"   {line}")
                        log_buffer.append(line)
                        last_log_time = time.time()
            except (IOError, OSError):
                # æ²¡æœ‰æ›´å¤šè¾“å‡ºï¼Œç»§ç»­ç­‰å¾…
                pass
            
            # å°è¯•è¿æ¥å¥åº·æ£€æŸ¥ç«¯ç‚¹
            try:
                response = requests.get(health_url, timeout=2)
                if response.status_code == 200:
                    print("-" * 60)
                    print("âœ… vLLM æœåŠ¡å·²å°±ç»ªï¼")
                    print(f"   æœåŠ¡åœ°å€: {base_url}")
                    print(f"   å¥åº·æ£€æŸ¥: {health_url}")
                    print()
                    print("ğŸ’¡ æç¤ºï¼šæœåŠ¡å°†åœ¨åå°è¿è¡Œï¼ŒæŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
                    return process
            except requests.exceptions.RequestException:
                pass
            
            # æ¯ 10 ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æç¤º
            elapsed = attempt * wait_interval
            if elapsed > 0 and elapsed % 10 == 0:
                print(f"   â³ ä»åœ¨å¯åŠ¨ä¸­... ({elapsed} ç§’)")
                if elapsed > 60:
                    print(f"   ğŸ’¡ é¦–æ¬¡å¯åŠ¨é€šå¸¸éœ€è¦ 1-3 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
            
            time.sleep(wait_interval)
        
        # è¶…æ—¶
        print(f"âŒ vLLM æœåŠ¡å¯åŠ¨è¶…æ—¶ï¼ˆ{max_wait} ç§’ï¼‰")
        print("   è¯·æ£€æŸ¥ï¼š")
        print("   1. GPU æ˜¯å¦å¯ç”¨")
        print("   2. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   3. ç«¯å£æ˜¯å¦è¢«å ç”¨")
        process.terminate()
        sys.exit(1)
    
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        if process.poll() is None:
            process.terminate()
        sys.exit(1)
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        if process.poll() is None:
            process.terminate()
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="å¯åŠ¨ vLLM æœåŠ¡ï¼ˆOpenAI-compatible APIï¼‰"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="æ¨¡å‹è·¯å¾„æˆ– HuggingFace IDï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="æœåŠ¡ç«¯å£ï¼ˆé»˜è®¤: 8000ï¼‰"
    )
    parser.add_argument(
        "--gpu-memory-utilization",
        type=float,
        default=0.7,  # é»˜è®¤ 70%
        help="GPU å†…å­˜ä½¿ç”¨ç‡ï¼ˆé»˜è®¤ 0.7ï¼Œå³ 70%ï¼‰"
    )
    parser.add_argument(
        "--max-model-len",
        type=int,
        default=1024,  # é»˜è®¤ 1024
        help="æœ€å¤§æ¨¡å‹é•¿åº¦ï¼ˆé»˜è®¤ 1024ï¼‰"
    )
    parser.add_argument(
        "--trust-remote-code",
        action="store_true",
        help="ä¿¡ä»»è¿œç¨‹ä»£ç "
    )
    parser.add_argument(
        "--enforce-eager",
        action="store_true",
        default=True,  # é»˜è®¤ç¦ç”¨ CUDA graphï¼ˆé¿å… duplicate template name é”™è¯¯ï¼‰
        help="ç¦ç”¨ CUDA graphï¼ˆé»˜è®¤å¯ç”¨ï¼Œé¿å… duplicate template name é”™è¯¯ï¼‰"
    )
    parser.add_argument(
        "--no-enforce-eager",
        dest="enforce_eager",
        action="store_false",
        help="å¯ç”¨ CUDA graphï¼ˆå¯èƒ½è§¦å‘ duplicate template name é”™è¯¯ï¼Œä½†æ€§èƒ½æ›´å¥½ï¼‰"
    )
    parser.add_argument(
        "--disable-prefix-caching",
        action="store_true",
        help="ç¦ç”¨å‰ç¼€ç¼“å­˜ï¼ˆPrefix Cachingï¼‰ã€‚é»˜è®¤å¯ç”¨å‰ç¼€ç¼“å­˜ä»¥æå‡æ€§èƒ½"
    )
    
    args = parser.parse_args()
    
    # å¯åŠ¨æœåŠ¡
    process = start_vllm_server(
        model=args.model,
        port=args.port,
        gpu_memory_utilization=args.gpu_memory_utilization,
        max_model_len=args.max_model_len,
        enforce_eager=args.enforce_eager,
        enable_prefix_caching=not args.disable_prefix_caching,  # é»˜è®¤å¯ç”¨
        trust_remote_code=args.trust_remote_code,
    )
    
    # æ³¨å†Œä¿¡å·å¤„ç†ï¼Œç¡®ä¿é€€å‡ºæ—¶æ¸…ç†
    def signal_handler(sig, frame):
        print("\nâš ï¸  æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æœåŠ¡...")
        if process.poll() is None:
            process.terminate()
            try:
                process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                process.kill()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # ç­‰å¾…è¿›ç¨‹ç»“æŸï¼ˆæˆ–ç”¨æˆ·ä¸­æ–­ï¼‰
    try:
        # å®æ—¶è¾“å‡ºæ—¥å¿—
        if process.stdout:
            for line in process.stdout:
                print(line, end='')
    except KeyboardInterrupt:
        signal_handler(signal.SIGINT, None)
    
    # ç­‰å¾…è¿›ç¨‹ç»“æŸ
    process.wait()


if __name__ == "__main__":
    main()
